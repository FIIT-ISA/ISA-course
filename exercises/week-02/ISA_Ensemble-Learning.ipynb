{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a76311-4bde-4cf1-b136-ff321cd9b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c29c4-0a62-4d7c-9088-d1dcf23df7a1",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "- Ensemble learning is an advanced technique in the field of machine learning that involves combining multiple models to achieve better results compared to individual models. \n",
    "- These models are often referred to as weak learners, and the intuition behind ensemble learning is that when several weak learners are combined, they can become strong learners.\n",
    "- Each weak learner is trained on the training set and provides predictions. The final prediction is then computed by combining the results from all the weak learners.\n",
    "\n",
    "### Basic Ensemble Learning Techniques:\n",
    "- Max Voting\n",
    "- Averaging\n",
    "- Weighted Average\n",
    "\n",
    "### Advanced ensemble learning techniques:\n",
    "- Stacking\n",
    "- Blending\n",
    "- Bagging\n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a02299-9770-425e-9667-b03217852bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39338c9a-7b7f-48ba-af00-bdd188708537",
   "metadata": {},
   "source": [
    "# Max Voting\n",
    "\n",
    "It is mainly used for classification problems. The Max Voting technique treats the predictions from each model as votes. The final prediction is determined by the prediction with the most votes.\n",
    "\n",
    "#### Theoretical example: \n",
    "\n",
    "Imagine you have a dataset with movie ratings from various film critics. You want to determine if a movie will be successful or not (e.g., whether it will make a lot of money at the box office or receive positive reviews).\n",
    "You have three classifiers, each trained on different data and using different approaches to evaluate movies:\n",
    "- **Classifier 1** - Predicts a successful movie.\n",
    "- **Classifier 2** - Predicts an unsuccessful movie.\n",
    "- **Classifier 3** - Predicts a successful movie.\n",
    "  \n",
    "If the majority of classifiers (in this case, 2 out of 3) predict the movie as successful, it is likely to be successful.\n",
    "\n",
    "#### Practical example:\n",
    "In this  example, three classification models (logistic regression, SVC, and random forest) are combined using sklearn VotingClassifier, that model is trained and the class with maximum votes is returned as output. The final prediction output is **prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b5b31-c5f4-4d24-9d1b-62437e7e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing machine learning models for prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importing voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize models with their default parameters\n",
    "model_lr = LogisticRegression(max_iter=10000)  # Increased max_iter for convergence\n",
    "model_svc = SVC(probability=True)  # Enable probability for soft voting\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "# Making the final model using voting classifier\n",
    "final_model = VotingClassifier(\n",
    "    estimators=[('lr', model_lr), ('svc', model_svc), ('rf', model_rf)], voting='soft')\n",
    "\n",
    "# Fit the final model on the training dataset\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the output on the test dataset\n",
    "prediction = final_model.predict(X_test)\n",
    " \n",
    "# Print accuracy between actual and predicted value\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(f'Accuracy of the final model: {accuracy:.2f}')\n",
    "# The accuracy score provides a straightforward interpretation of how well the combined model performs.\n",
    "# An accuracy of 1.0 means the model is perfect at classifying the digits in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8290d7-0d93-4fe9-a715-9fa5f6714c79",
   "metadata": {},
   "source": [
    "# Averaging\n",
    "\n",
    "It is mainly used for regression problems. The Averaging technique calculates the final output as the average of all predictions. For instance, in random forest regression, the final result is the average of the predictions from individual decision trees.\n",
    "\n",
    "#### Theoretical example: \n",
    "\n",
    "Suppose you have three different temperature sensors in a particular location, but each sensor has some degree of error. Each sensor provides temperature values at a given moment:\n",
    "- **Sensor 1** - Records 20°C.\n",
    "- **Sensor 2** - Records 22°C.\n",
    "- **Sensor 3** - Records 19°C.\n",
    "\n",
    "When predicting the actual temperature, the result is obtained by averaging the values from all sensors:\n",
    "\n",
    "(20 + 22 + 19) / 3 = 20.33°C. \n",
    "\n",
    "This allows us to obtain a more accurate estimate of the current temperature.\n",
    "\n",
    "#### Practical example:\n",
    "In this  example, three regression models (linear regression, xgboost, and random forest) are trained and their predictions are averaged (using the California housing dataset). The final prediction output is **prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a2cb2-50fc-454b-befb-9b42f72d37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "df = california_housing.frame\n",
    "\n",
    "# The target variable is already continuous and suitable for regression\n",
    "X = df.drop('MedHouseVal', axis=1)  # Features\n",
    "y = df['MedHouseVal']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize models with their default parameters\n",
    "model_Lr = LinearRegression()\n",
    "model_Xgb = xgb.XGBRegressor(objective ='reg:squarederror')  # Specify objective for regression\n",
    "model_Rfr = RandomForestRegressor()\n",
    "\n",
    "# Train all the models on the training data\n",
    "model_Lr.fit(X_train, y_train)\n",
    "model_Xgb.fit(X_train, y_train)\n",
    "model_Rfr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation dataset\n",
    "pred_Lr = model_Lr.predict(X_test)\n",
    "pred_Xgb = model_Xgb.predict(X_test)\n",
    "pred_Rfr = model_Rfr.predict(X_test)\n",
    "\n",
    "# Final prediction after averaging the predictions of all 3 models\n",
    "prediction = (pred_Lr + pred_Xgb + pred_Rfr) / 3.0\n",
    "\n",
    "# Print the mean squared error between the actual values and predicted values\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "# A lower MSE value indicates that the model is more accurate and that your predictions are closer to the actual values.\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383bce2c-3f1a-4e60-bd53-3d397a69da5a",
   "metadata": {},
   "source": [
    "# Weighted Average\n",
    "\n",
    "In weighted averaging, the base model with higher predictive power is given more importance. In the price prediction example, each of the regressors is assigned a weight. The sum of the weights equals one.\n",
    "\n",
    "#### Theoretical example:\n",
    "\n",
    "Imagine you are trying to predict stock prices in the market. You have three different models, and each model has a different track record of success in predicting stock prices.\n",
    "The models are as follows:\n",
    "- **Model 1** - Accuracy 60%.\n",
    "- **Model 2** - Accuracy 75%.\n",
    "- **Model 3** - Accuracy 80%.\n",
    "  \n",
    "In this case, you can assign weights to each model based on its accuracy. For example:\n",
    "\n",
    "- **Model 1** - Weight 0.2.\n",
    "- **Model 2** - Weight 0.3.\n",
    "- **Model 3** - Weight 0.5.\n",
    "\n",
    "The final stock price can be calculated using a weighted average prediction:\n",
    "\n",
    "(0.2 * Model 1 Prediction) + (0.3 * Model 2 Prediction) + (0.5 * Model 3 Prediction) = Final Stock Price Prediction.\n",
    "\n",
    "This way, you can effectively combine predictions from different models with varying levels of accuracy in predicting stock prices.\n",
    "\n",
    "#### Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bfd8a-9780-4acf-b8a5-7e27c335635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Assuming 'MedHouseVal' is the target variable\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Initialize models with their default parameters\n",
    "model_Lr = LinearRegression()\n",
    "model_Xgb = xgb.XGBRegressor(objective='reg:squarederror')  # Specify the objective to avoid warnings\n",
    "model_Rfr = RandomForestRegressor()\n",
    "\n",
    "# Train all the models on the training data\n",
    "model_Lr.fit(X_train, y_train)\n",
    "model_Xgb.fit(X_train, y_train)\n",
    "model_Rfr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation dataset\n",
    "pred_Lr = model_Lr.predict(X_test)\n",
    "pred_Xgb = model_Xgb.predict(X_test)\n",
    "pred_Rfr = model_Rfr.predict(X_test)\n",
    "\n",
    "# Weights for the models, determined based on their performance\n",
    "# You can try changing the weights and see how it will change prediction and MSE, but ensure the sum of weights is 1\n",
    "weights = {'Lr': 0.5, 'Xgb': 0.1, 'Rfr': 0.4}\n",
    "\n",
    "# Calculate the weighted average of predictions\n",
    "weighted_prediction = (pred_Lr * weights['Lr'] +\n",
    "                       pred_Xgb * weights['Xgb'] +\n",
    "                       pred_Rfr * weights['Rfr'])\n",
    "\n",
    "# Calculate and print the MSE for the weighted average\n",
    "weighted_mse = mean_squared_error(y_test, weighted_prediction)\n",
    "print(\"Mean Squared Error for the weighted average:\", weighted_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681ce9c-44b4-45a3-8365-3686fd490b85",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Stacking is the process of combining various estimators in order to reduce their biases. Predictions from each estimator are stacked together and used as input to a final estimator (usually called a meta-model) that computes the final prediction. Training of the final estimator happens via cross-validation. Stacking can be done for both regression and classification problems.\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "Stacking can be considered to happen in the following steps:\n",
    "1. Split the train dataset into n parts\n",
    "2. A base model (say linear regression) is fitted on n-1 parts and predictions are made for the nth part. This is done for each one of the n part of the train set.\n",
    "3. The base model is then fitted on the whole train dataset.\n",
    "4. This model is used to predict the test dataset\n",
    "5. The Steps 2 to 4 are repeated for another base model which results in another set of predictions for the train and test dataset.\n",
    "6. The predictions on train data set are used as a feature to build the new model.\n",
    "7. This final model is used to make the predictions on test dataset.\n",
    "\n",
    "#### Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a308f9b-4991-40c3-8bd3-e1a977589450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Assuming 'MedHouseVal' is the target variable\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=10, random_state=42)),\n",
    "    ('svr', SVR(C=1, gamma='auto'))\n",
    "]\n",
    "\n",
    "# Define meta-learner model\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Create the stacking model\n",
    "stacked_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Train the stacking model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation dataset\n",
    "stacked_prediction = stacked_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the MSE for the stacking model\n",
    "stacking_mse = mean_squared_error(y_test, stacked_prediction)\n",
    "print(\"Mean Squared Error for the Stacking Model:\", stacking_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca84fcb-d5c8-4f27-b536-631fe8e8d04a",
   "metadata": {},
   "source": [
    "# Blending\n",
    "\n",
    "Blending is similar to stacking, but uses a holdout set from the training set to make predictions. So, predictions are done on the holdout set only. The predictions and holdout set are used to build a final model that makes predictions on the test set. You can think of blending as a type of stacking, where the meta-model is trained on predictions made by the base model on the hold-out validation set.\n",
    "\n",
    "#### Algorithm:\n",
    "You can consider the blending process to be:\n",
    "1. Split the data into a test and validation set.\n",
    "2. Fit base models on the validation set.\n",
    "3. Make predictions on the validation and test set.\n",
    "4. Use the validation set and its predictions to build a final model.\n",
    "5. Make final predictions using this model.\n",
    "\n",
    "#### Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217a16d-b7f6-4a2c-90f3-28cf2fa0a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Assuming 'MedHouseVal' is the target variable\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split the data into training, validation, and holdout sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_valid, X_holdout, y_valid, y_holdout = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=10, random_state=42)),\n",
    "    ('svr', SVR(C=1, gamma='auto'))\n",
    "]\n",
    "\n",
    "# Define meta-learner model\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Train and create predictions for each base model, and stack the predictions together for the holdout set\n",
    "base_predictions_holdout = np.hstack([\n",
    "    model.fit(X_train, y_train).predict(X_holdout).reshape(-1,1)\n",
    "    for _, model in base_models\n",
    "])\n",
    "\n",
    "# Convert the stacked array to DataFrame for holdout set\n",
    "blend_data_holdout = pd.DataFrame(base_predictions_holdout)\n",
    "\n",
    "# Train meta-learner on holdout predictions\n",
    "meta_model.fit(blend_data_holdout, y_holdout)\n",
    "\n",
    "# Generate and stack predictions for each base model on validation set\n",
    "base_predictions_valid = np.hstack([\n",
    "    model.predict(X_valid).reshape(-1,1)\n",
    "    for _, model in base_models\n",
    "])\n",
    "\n",
    "# Convert the stacked array to DataFrame for validation set\n",
    "blend_data_valid = pd.DataFrame(base_predictions_valid)\n",
    "\n",
    "# Predict on the validation dataset using meta-learner\n",
    "blended_prediction = meta_model.predict(blend_data_valid)\n",
    "\n",
    "# Calculate and print the MSE for the blended model\n",
    "blending_mse = mean_squared_error(y_valid, blended_prediction)\n",
    "print(\"Mean Squared Error for the Blending Model:\", blending_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8e958-1fa8-4bcf-a53f-0c84a5aae4b7",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique in machine learning. It aims to improve the accuracy and stability of a predictive model by reducing variance and mitigating overfitting. Bagging works by creating multiple subsets (bags) of the training data using bootstrapping and training a separate base model on each subset. The final prediction is typically obtained by aggregating the predictions of these base models.\n",
    "\n",
    "#### Algorithm:\n",
    "The method involves:\n",
    "1. Creating multiple subsets from the original dataset with replacement.\n",
    "2. Building a base model for each of the subsets.\n",
    "3. Running all the models in parallel.\n",
    "4. Combining predictions from all models to obtain final predictions.\n",
    "\n",
    "#### Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2ead0-205b-4cc1-9966-7d9060f8aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Assuming 'MedHouseVal' is the target variable\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Define the base model for bagging\n",
    "# Create a Bagging ensemble of Decision Trees\n",
    "bagging_model = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the bagging ensemble\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "bagging_prediction = bagging_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the MSE for the bagging model\n",
    "bagging_mse = mean_squared_error(y_test, bagging_prediction)\n",
    "print(\"Mean Squared Error for the Manual Bagging Model:\", bagging_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc10b6-4c57-4f22-907b-2ace7d0d8493",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting is an ensemble learning technique in machine learning that focuses on converting a set of weak learners into a strong learner. It does this by sequentially training the weak learners and giving more weight to examples that the previous learners found difficult to classify correctly. This process continues iteratively, allowing each weak learner to fix the errors made by the previous ones. The final prediction is a weighted combination of the individual learners' predictions.\n",
    "\n",
    "#### Algorithm:\n",
    "Here’s what the entire process looks like:\n",
    "1. Take a subset of the train dataset.\n",
    "2. Train a base model on that dataset.\n",
    "3. Use third model to make predictions on the whole dataset.\n",
    "4. Calculate errors using the predicted values and actual values.\n",
    "5. Initialize all data points with same weight.\n",
    "6. Assign higher weight to incorrectly predicted data points.\n",
    "7. Make another model, make predictions using the new model in such a way that errors made by the previous model are mitigated/corrected.\n",
    "8. Similarly, create multiple models–each successive model correcting the errors of the previous model.\n",
    "9. The final model (strong learner) is the weighted mean of all the previous models (weak learners).\n",
    "\n",
    "#### Practical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a75cb-3502-4574-9f1d-63155ba06922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing utility modules\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing machine learning models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Assuming 'MedHouseVal' is the target variable\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Define the boosting model\n",
    "boosting_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the boosting model\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "boosting_prediction = boosting_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the MSE for the boosting model\n",
    "boosting_mse = mean_squared_error(y_test, boosting_prediction)\n",
    "print(\"Mean Squared Error for the Boosting Model:\", boosting_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
