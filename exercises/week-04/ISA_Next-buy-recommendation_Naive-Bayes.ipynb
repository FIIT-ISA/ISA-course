{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08345e-a288-4a70-b3b9-6cd88b528976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ada4e-363b-4971-a676-ddd274bc3890",
   "metadata": {},
   "source": [
    "# Next buy item prediction with Naive Bayes\n",
    "\n",
    "## Practical Implementation of Naive Bayes for Next Buy Item Prediction\n",
    "\n",
    "- This practical implementation demonstrates the use of the Naive Bayes algorithm to predict whether users of a social network platform will make a purchase based on their profile. \n",
    "- The objective is to construct a Naive Bayes model that can accurately predict purchase behavior and to compare the performance of models - using single versus multiple features.\n",
    "\n",
    "We use **GaussianNB** because it assumes that the features follow a Gaussian distribution. It is particularly well-suited for continuous data and is a common assumption for real-world data.\n",
    "\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "**Dataset from: https://www.kaggle.com/datasets/akram24/social-network-ads**\n",
    "\n",
    "**Features:**\n",
    "- **User ID:** Unlikely to be predictive of the outcome, will be dropped from the dataset.\n",
    "- **Gender:** Categorical feature that will be numerically encoded.\n",
    "- **Age:** Continuous feature that may influence purchasing behavior.\n",
    "- **EstimatedSalary:** Continuous feature representing economic power, potentially predictive of purchasing behavior.\n",
    "- **Purchased:** Binary target variable indicating whether a purchase was made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91d628-98bd-4267-8287-180a1a4aee57",
   "metadata": {},
   "source": [
    "### Steps for Implementation\n",
    "1. **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70626b-4f9c-4baa-922e-f7343a19474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = pd.read_csv('data/Social_Network_Ads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3939c23-e7a3-49df-b9f1-6d124c5515ab",
   "metadata": {},
   "source": [
    "2. **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43214b-e65d-4b1b-9d9c-dc58cfbdb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the dataset\n",
    "# Convert 'Gender' to a binary numerical variable\n",
    "encoder = LabelEncoder()\n",
    "data['Gender'] = encoder.fit_transform(data['Gender'])\n",
    "\n",
    "# Drop 'User ID' as it is not a useful feature for prediction\n",
    "data.drop('User ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cbb13-aec7-45e9-9cc7-b2a7885bcae9",
   "metadata": {},
   "source": [
    "3. **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadf147-9f02-4fa9-8a14-b79da61c3eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Selecting the target variable and feature set for both single and multiple feature models\n",
    "y = data['Purchased']\n",
    "X_single = data[['Age']]  # Single feature\n",
    "X_multiple = data[['Gender', 'Age', 'EstimatedSalary']]  # Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb568d-043c-484f-ac08-f59754bf0623",
   "metadata": {},
   "source": [
    "4. **Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fd626-e9d8-42c0-bd7e-b3e762d8a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Splitting the dataset into training and test sets for both single and multiple feature models.\n",
    "X_train_single, X_test_single, y_train, y_test = train_test_split(X_single, y, test_size=0.25, random_state=0)\n",
    "X_train_multiple, X_test_multiple, y_train, y_test = train_test_split(X_multiple, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773eab5-f66b-46a7-aad7-67abaf07e062",
   "metadata": {},
   "source": [
    "5. **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37251b28-2420-48f0-b5fe-ab079aa62adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Creating a Gaussian Naive Bayes classifier and training it on the single feature dataset and on the multiple features dataset.\n",
    "gnb_single = GaussianNB()\n",
    "gnb_single.fit(X_train_single, y_train)\n",
    "\n",
    "gnb_multiple = GaussianNB()\n",
    "gnb_multiple.fit(X_train_multiple, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245ee94-6f74-4f04-8b45-8f47d2fd6fe1",
   "metadata": {},
   "source": [
    "6. **Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d86d9-16b7-4cf9-b2ce-b4f114421892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Predicting and evaluating the single feature model.\n",
    "y_pred_single = gnb_single.predict(X_test_single)\n",
    "print(\"Single Feature Model\")\n",
    "print(confusion_matrix(y_test, y_pred_single))\n",
    "print(classification_report(y_test, y_pred_single))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_single)}\\n\")\n",
    "\n",
    "# Predicting and evaluating the multiple feature model.\n",
    "y_pred_multiple = gnb_multiple.predict(X_test_multiple)\n",
    "print(\"Multiple Features Model\")\n",
    "print(confusion_matrix(y_test, y_pred_multiple))\n",
    "print(classification_report(y_test, y_pred_multiple))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_multiple)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe87cf4-7c6f-4e49-a6f6-942aa00f7bee",
   "metadata": {},
   "source": [
    "### Conclusion and Interpretation of Model Performance\n",
    "\n",
    "Upon analyzing the performance metrics of the Naive Bayes classifier with single and multiple features, we notice an interesting outcome. Both models have yielded an accuracy score of 90%. This indicates that for the given dataset, adding more features did not significantly increase the accuracy of the model. However, there are subtle differences in other metrics that are worth noting.\r\n",
    "\r\n",
    "For the single feature model (using 'Age' alone), we observe that while the precision for predicting a 'No' purchase (class 0) is slightly lower than that for a 'Yes' purchase (class 1), the recall for class 0 is considerably higher than that for class 1. This means the single feature model is more conservative, favoring predictions of 'No' purchase, which could be beneficial in scenarios where falsely predicting a purchase is costlier than missing a potential purchase.\r\n",
    "\r\n",
    "The multiple features model, on the other hand, shows a balanced recall, indicating a slight improvement in identifying 'Yes' purchases. The model with multiple features is better-rounded, not significantly biased towards either class, which could be advantageous when a balance between predicting 'Yes' and 'No' purchases is desired.\r\n",
    "\r\n",
    "In practice, the choice between a single feature and multiple features model would depend on the specific requirements of the application. If a simple model is needed, with fewer computations and the ease of interpretation, a single feature might suffice. However, if the context requires a more comprehensive analysis, incorporating multiple features could provide a nuanced understanding, even if the accuracy remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d16dd-1da3-49f4-9840-7dab337d82fb",
   "metadata": {},
   "source": [
    "# Theoretical Introduction to Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a probabilistic machine learning model that is used for classification tasks, the fundamental task of assigning a label to a given input sample. This model is particularly known for its simplicity and effectiveness in handling large datasets.\n",
    "\n",
    "Naive Bayes classifiers are based on Bayes' Theorem, which uses the probabilities of events to make predictions. The 'naive' aspect of the classifier comes from the assumption that the features used to make the classification are mutually independent from each other.\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem is stated mathematically as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `P(A|B)` is the posterior probability of class `A` given predictor `B`.\n",
    "- `P(B|A)` is the likelihood which is the probability of predictor `B` given class `A`.\n",
    "- `P(A)` is the prior probability of class `A`.\n",
    "- `P(B)` is the prior probability of predictor `B`.\n",
    "\n",
    "### How Naive Bayes Works?\n",
    "\n",
    "Naive Bayes classifier computes the posterior probability of each class based on Bayes' Theorem, and the class with the highest posterior probability is the outcome of prediction. The steps involved in this process are:\n",
    "\n",
    "1. **Calculate the prior probabilities:** Prior probability of each class (e.g., spam or not spam) is calculated by dividing the number of samples in each class by the total number of samples.\n",
    "2. **Calculate the likelihood:** Probability of each feature in each class is calculated.\n",
    "3. **Compute the posterior probability for each class:** The likelihood of each class given an input sample is multiplied by the prior of that class. The result is normalized by dividing by the probability of the data.\n",
    "4. **Classify the sample based on the highest posterior probability.**\n",
    "\n",
    "Now that we have a foundational understanding of Naive Bayes classification and how it operates based on Bayes' Theorem, let's explore its application through theoretical examples. These will illustrate how the algorithm uses probability to make predictions, whether we are considering a single feature or multiple features to determine an outcome.\n",
    "\n",
    "### Various types of Naive Bayes classifiers\n",
    "\n",
    "After exploring how the Naive Bayes classifier works with a theoretical example considering both single and multiple features, we can further delve into the various types of Naive Bayes classifiers provided by Scikit-Learn, each suited for different data distributions:\n",
    "\n",
    "- **GaussianNB:** Used in classification and it assumes that features follow a normal distribution. This is best when features are continuous and not categorical.\n",
    "- **BernoulliNB:** Designed for binary/boolean features, this classifier is useful when your features are binary or take on only two values.\n",
    "- **MultinomialNB:** This is ideal for features that are counts or count rates, commonly used in text classification.\n",
    "- **ComplementNB:** An adaptation of the standard MultinomialNB that is particularly suited for imbalanced data sets.\n",
    "- **CategoricalNB:** Appropriate for categorical data, this classifier assumes that each feature has its own Bernoulli distribution.\n",
    "\n",
    "By selecting the appropriate Naive Bayes model for our data, we enhance the classifier's accuracy and reliability. Each type takes advantage of different data characteristics, which can significantly affect the outcome of our predictions. Let's apply these classifiers to our dataset and observe their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daecafc-ce9c-46ad-a16a-b74db2a1ccb9",
   "metadata": {},
   "source": [
    "# Single Feature Example: \n",
    "## Purchase Likelihood Based on Product Price Range\n",
    "\n",
    "Initially, we'll start with a single feature example, where we focus solely on the price range of products. Imagine we have a dataset on customer purchases with respect to the price range of the products they bought. Our goal is to predict the likelihood of a customer making a subsequent purchase based on the price range of the item. In this example, we focus on the '`Budget`' price range to predict if customers will make another purchase.\n",
    "\n",
    "Our dataset includes instances of purchases across various price ranges:\n",
    "\n",
    "| Price Range | Made Purchase |\n",
    "|:------------|:--------------|\n",
    "| Budget      | Yes           |\n",
    "| Premium     | No            |\n",
    "| Budget      | Yes           |\n",
    "| Midrange    | No            |\n",
    "| Premium     | Yes           |\n",
    "| Budget      | No            |\n",
    "| Midrange    | Yes           |\n",
    "\n",
    "We aim to assess the probability of a customer making another purchase after buying a '`Budget`' range item.\n",
    "\n",
    "### Process:\n",
    "\n",
    "- **Step 1: Calculate Prior Probabilities**\n",
    "  \n",
    "We begin by calculating the prior probability of both possible outcomes—making another purchase (`Yes`) and not making another purchase (`No`).\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}) = \\frac{\\text{Number of 'Yes' purchases}}{\\text{Total purchases}} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No}) = \\frac{\\text{Number of 'No' purchases}}{\\text{Total purchases}} = \\frac{3}{7}\n",
    "$$\n",
    "\n",
    "- **Step 2: Calculate Likelihood Probabilities**\n",
    "  \n",
    "Next, the likelihood of customers making another purchase after previously selecting a '`Budget`' range product is calculated.\n",
    "\n",
    "$$\n",
    "P(\\text{Budget}|\\text{Yes}) = \\frac{\\text{Number of 'Yes' purchases within 'Budget'}}{\\text{Total 'Yes' purchases}} = \\frac{2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Budget}|\\text{No}) = \\frac{\\text{Number of 'No' purchases within 'Budget'}}{\\text{Total 'No' purchases}} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "- **Step 3: Calculate Posterior Probabilities**\n",
    "  \n",
    "Employing Bayes' Theorem, we determine the posterior probabilities for a customer making a subsequent purchase (`Yes`) and not making one (`No`), given the product was in the '`Budget`' category:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}|\\text{Budget}) = \\frac{P(\\text{Budget}|\\text{Yes}) \\times P(\\text{Yes})}{P(\\text{Budget})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No}|\\text{Budget}) = \\frac{P(\\text{Budget}|\\text{No}) \\times P(\\text{No})}{P(\\text{Budget})}\n",
    "$$\n",
    "\n",
    "Assuming the probability of selecting a '`Budget`' item `P(Budget)` is calculated as the number of '`Budget`' purchases over the total purchases:\n",
    "\n",
    "$$\n",
    "P(\\text{Budget}) = \\frac{3}{7}\n",
    "$$\n",
    "\n",
    "The posterior probabilities are therefore:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}|\\text{Budget}) = \\frac{2}{4} \\times \\frac{4}{7} = \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No}|\\text{Budget}) = \\frac{1}{3} \\times \\frac{3}{7} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "- **Step 4: Make the Prediction**\n",
    "  \n",
    "We make our prediction by comparing the two posterior probabilities. With `P(Yes|Budget)` being greater than `P(No|Budget)`, we predict that customers are likely to make another purchase if their previous purchase was a '`Budget`' item\n",
    "\n",
    "### Conclusion\n",
    "  \n",
    "The analysis conducted with the Naive Bayes classifier points to a higher likelihood of repeat purchases within the '`Budget`' price category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838d53b-3c86-47df-ba9c-3742433fbe6e",
   "metadata": {},
   "source": [
    "# Multiple Features Example\n",
    "## Purchase Likelihood Based on Product Price Range and Category\n",
    "\n",
    "Building on the single feature scenario, we'll then proceed to a more realistic situation where multiple features influence the likelihood of a customer making a purchase. In this example, we'll predict the likelihood of a customer making a next purchase based on two features: the '`Budget`' price range and '`Electronics`' category.\n",
    "\n",
    "Consider a dataset that includes records of purchases with the added dimension of product category:\n",
    "\n",
    "| Price Range | Category    | Made Purchase |\n",
    "|-------------|-------------|---------------|\n",
    "| Budget      | Electronics | Yes           |\n",
    "| Premium     | Clothing    | No            |\n",
    "| Budget      | Home Goods  | Yes           |\n",
    "| Midrange    | Electronics | No            |\n",
    "| Premium     | Home Goods  | Yes           |\n",
    "| Budget      | Clothing    | No            |\n",
    "| Midrange    | Clothing    | Yes           |\n",
    "\n",
    "\n",
    "We want to calculate the likelihood of a customer making another purchase if the item is a '`Budget`' electronics product.\n",
    "\n",
    "### Process:\n",
    "- **Step 1: Calculate Prior Probabilities**\n",
    "\n",
    "Firstly, we establish the prior probabilities of the two possible outcomes—making another purchase (`Yes`) and not making another purchase (`No`).\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}) = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No}) = \\frac{3}{7}\n",
    "$$\n",
    "\n",
    "- **Step 2: Calculate Likelihood Probabilities**\n",
    "\n",
    "We then calculate the likelihood of a customer making another purchase given the combined features of '`Budget`' price range and '`Electronics`' category.\n",
    "\n",
    "$$\n",
    "P(\\text{Budget} \\cap \\text{Electronics}|\\text{Yes}) = \\frac{1}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{Budget} \\cap \\text{Electronics}|\\text{No}) = \\frac{0}{3}\n",
    "$$\n",
    "\n",
    "- **Step 3: Calculate Posterior Probabilities**\n",
    "\n",
    "Applying Bayes' Theorem, we derive the posterior probabilities, considering the total probability of a '`Budget Electronics`' product being purchased by summing the instances of purchases over the total number of observations:\n",
    "\n",
    "$$\n",
    "P(\\text{Budget} \\cap \\text{Electronics}) = \\frac{1}{7}\n",
    "$$\n",
    "\n",
    "The posterior probabilities are thus:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}|\\text{Budget} \\cap \\text{Electronics}) = \\frac{P(\\text{Budget} \\cap \\text{Electronics}|\\text{Yes}) \\times P(\\text{Yes})}{P(\\text{Budget} \\cap \\text{Electronics})} = \\frac{1}{4} \\times \\frac{4}{7} \\div \\frac{1}{7} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No}|\\text{Budget} \\cap \\text{Electronics}) = \\frac{P(\\text{Budget} \\cap \\text{Electronics}|\\text{No}) \\times P(\\text{No})}{P(\\text{Budget} \\cap \\text{Electronics})} = \\frac{0}{3} \\times \\frac{3}{7} \\div \\frac{1}{7} = 0\n",
    "$$\n",
    "\n",
    "- **Step 4: Make the Prediction**\n",
    "\n",
    "Considering our calculated probabilities, the prediction is straightforward. Since `P(Yes|Budget ∩ Electronics)` is greater than `P(No|Budget ∩ Electronics)`, we predict that customers are certain to make another purchase if they have previosly bought a '`Budget`' electronics item.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The computed outcome from the Naive Bayes classifier suggests a strong likelihood of repeat purchases within the 'Budget Electronics' category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
