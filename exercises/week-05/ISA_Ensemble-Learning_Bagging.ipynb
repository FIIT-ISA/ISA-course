{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d88e8-5aea-429d-b9d5-a7df694eb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af751b0-eed7-483f-aa81-dc51874f167b",
   "metadata": {},
   "source": [
    "# Bagging algorithms\n",
    "- Random Forest\n",
    "- Extra-Trees (Extremely Randomized Trees)\n",
    "- Bagging Meta-Estimator\n",
    "- their comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08819b1-de66-40b2-82f4-775b2649028b",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "A Random Forest is an ensemble learning method predominantly used for classification and regression. It builds multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random Forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "Here's how the Random Forest algorithm typically works:\n",
    "- **Step 1: Create Bootstrap Samples -** Random Forest starts by creating multiple bootstrap samples from the original dataset. A bootstrap sample is a sample taken with replacement.\n",
    "- **Step 2: Build Decision Trees -** For each bootstrap sample, it grows a decision tree. When splitting a node during the construction of the tree, the chosen split is no longer the best among all features. Instead, the split that is best among a random subset of features is chosen. This adds a layer of randomness to the model, hence the name.\n",
    "- **Step 3: Node Splitting -** Typically, for a classification problem with p features, âˆšp (rounded down) features are used in each split. For regression, p/3 features are used. This ensures that the individual trees in the forest are de-correlated.\n",
    "- **Step 4: Final Prediction -** Predictions for new data points are made by averaging the predictions of all the individual trees for regression or by majority vote for classification.\n",
    "\n",
    "Here are some reasons why Random Forest is powerful:\n",
    "- **Reduces Overfitting:** It handles overfitting by averaging or taking the majority vote of the predictions of individual trees which may have overfitted the data.\n",
    "- **Handles Missing Values:** It can handle missing values in the data.\n",
    "- **Automatic Feature Selection:** It gives estimates of what variables are important in the classification.\n",
    "- **Flexibility:** It can perform both classification and regression tasks.\n",
    "- **Easy to Use:** It has few hyperparameters to tune and can often work well with the default settings.\n",
    "\n",
    "### Practical Example of Using Random Forest:\n",
    "\n",
    "In this practical session, we will delve into the Random Forest algorithm and our objectives are to:\n",
    "1. Understand the basics of the Random Forest algorithm and its strengths.\n",
    "2. Apply Random Forest to a real-world dataset and evaluate its performance.\n",
    "3. Contrast the performance of Random Forest with that of a single decision tree to demonstrate the advantages of ensemble learning.\n",
    "4. Visualize the impact of Random Forest on feature importance and potentially decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8953f19a-3c8c-4888-80d1-0bab32203b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # seaborn is used for enhanced visual representation of data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "# Preprocessing: encoding categorical variables, handling missing values\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare data for training and testing\n",
    "X = df.drop('label', axis=1)  # Feature matrix\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Decision Tree Classifier for a simple baseline comparison\n",
    "single_tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "single_tree_clf.fit(X_train, y_train)\n",
    "y_tree_pred = single_tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_tree_pred) * 100\n",
    "\n",
    "# Initialize the Random Forest Classifier with default settings for quick comparison\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier and measure the training time\n",
    "start_time = time.time()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "print(f\"Random Forest Training Time: {rf_train_time:.3f} seconds\")\n",
    "\n",
    "# Predict with the Random Forest Classifier and measure the prediction time\n",
    "start_time = time.time()\n",
    "y_rf_pred = rf_clf.predict(X_test)\n",
    "rf_predict_time = time.time() - start_time\n",
    "print(f\"Random Forest Prediction Time: {rf_predict_time:.3f} seconds\")\n",
    "\n",
    "# Accuracy evaluation for the Random Forest Classifier\n",
    "rf_accuracy = accuracy_score(y_test, y_rf_pred)*100\n",
    "\n",
    "# Print accuracy scores for both the simple decision tree and the random forest\n",
    "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.2f}%\")\n",
    "print(f\"Random Forest Model Accuracy: {rf_accuracy:.2f}%\")\n",
    "\n",
    "# Generate and print classification reports\n",
    "tree_report = classification_report(y_test, y_tree_pred, zero_division=1)\n",
    "rf_report = classification_report(y_test, y_rf_pred, zero_division=1)\n",
    "print(\"Single Decision Tree Classification Report:\")\n",
    "print(tree_report)\n",
    "print(\"Random Forest Model Classification Report:\")\n",
    "print(rf_report)\n",
    "\n",
    "# Visualization 1: Comparing Model Accuracies\n",
    "# This bar chart makes it easier to compare the accuracy of the two models side by side.\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Single Decision Tree', 'Random Forest'], [tree_accuracy, rf_accuracy], color=['orange', 'forestgreen'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Confusion Matrix Heatmap\n",
    "# The confusion matrix is an important metric to understand the performance of classification models. It shows the number of correct and incorrect predictions made by the model, divided by each class.\n",
    "conf_matrix = confusion_matrix(y_test, y_rf_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Random Forest')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# After fitting the RandomForest model, extract the feature importances\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Visualization 3: Plotting the feature importances provides insight into which features the model finds most important\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "        color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and interpretation of results\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"Comparing the two models, the Random Forest Classifier typically outperforms a single decision tree, benefiting from the ensemble approach.\")\n",
    "print(\"The confusion matrix is particularly useful for visualizing the performance of the classifier across different classes, helping to identify where the classifier is making mistakes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8f12e-fb9f-4491-8116-efd7d52c1bb2",
   "metadata": {},
   "source": [
    "# Extra-Trees (Extremely Randomized Trees)\n",
    "\n",
    "Extra-Trees is an ensemble learning method similar to Random Forest, but it introduces more randomness into the construction of the individual trees. While Random Forest uses bootstrapping and chooses the best split among a subset of features at each node, Extra-Trees goes a step further by using the entire original sample and randomly selecting both the features and the split points for each feature, regardless of the outcome.\n",
    "\n",
    "Hereâ€™s how the Extra-Trees algorithm typically works:\n",
    "- **Step 1: Original Sample -** Unlike Random Forest, which creates bootstrap samples, Extra-Trees uses the entire original dataset to build each tree. This means that each tree in the Extra-Trees ensemble uses the full dataset rather than a bootstrap sample.\n",
    "- **Step 2: Random Splits -** When constructing the trees, for each feature at every node, a random split point is chosen, not necessarily the best one as in Random Forest. This increases the diversity among the trees at the cost of a slight increase in bias.\n",
    "- **Step 3: Building the Trees -** Extra-Trees builds multiple decision trees with the aforementioned method of random splits. No pruning is typically done, meaning the trees are grown to their maximum length.\n",
    "- **Step 4: Averaging/Majority Voting -** Similar to Random Forest, for regression problems, the final prediction is the average of the predictions of all the individual trees. For classification, it is the majority vote.\n",
    "\n",
    "Reasons why Extra-Trees can be powerful:\n",
    "- **Increased Randomness:** By randomizing the cut-points and using the whole dataset, Extra-Trees can create a more diverse ensemble, which can increase the accuracy for some datasets.\n",
    "- **Computational Efficiency:** Since it selects splits randomly, it can be faster to train compared to models that need to find the optimal splits.\n",
    "- **Reduction of Variance:** Like Random Forest, it helps in reducing variance by averaging the results, which can help with overfitted decision trees.\n",
    "- **No Need for Bootstrap Samples:** This can lead to a more efficient use of the data and sometimes better performance because the full dataset is used.\n",
    "\n",
    "### Practical Example of Using Extra-Trees:\n",
    "In practice, we can apply Extra-Trees to the same dataset used for the Random Forest to compare their performances later. By evaluating metrics like accuracy and feature importance, we can see how Extra-Trees performs in terms of prediction and understanding which features it deems most significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69089e-083a-4166-9bb2-6e987f6be718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # seaborn is used for enhanced visual representation of data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "# Preprocessing: encoding categorical variables, handling missing values\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare data for training and testing\n",
    "X = df.drop('label', axis=1)  # Feature matrix\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Decision Tree Classifier for a simple baseline comparison\n",
    "single_tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "single_tree_clf.fit(X_train, y_train)\n",
    "y_tree_pred = single_tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_tree_pred) * 100\n",
    "\n",
    "# Initialize the Extra Trees Classifier with default settings for quick comparison\n",
    "et_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Extra Trees Classifier and measure the training time\n",
    "start_time = time.time()\n",
    "et_clf.fit(X_train, y_train)\n",
    "et_train_time = time.time() - start_time\n",
    "print(f\"Extra Trees Training Time: {et_train_time:.3f} seconds\")\n",
    "\n",
    "# Predict with the Extra Trees Classifier and measure the prediction time\n",
    "start_time = time.time()\n",
    "y_et_pred = et_clf.predict(X_test)\n",
    "et_predict_time = time.time() - start_time\n",
    "print(f\"Extra Trees Prediction Time: {et_predict_time:.3f} seconds\")\n",
    "\n",
    "# Accuracy evaluation for the Extra Trees Classifier\n",
    "et_accuracy = accuracy_score(y_test, y_et_pred) * 100\n",
    "\n",
    "# Print accuracy scores for both the simple decision tree and the extra trees\n",
    "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.2f}%\")\n",
    "print(f\"Extra Trees Model Accuracy: {et_accuracy:.2f}%\")\n",
    "\n",
    "# Generate and print classification reports\n",
    "tree_report = classification_report(y_test, y_tree_pred, zero_division=1)\n",
    "et_report = classification_report(y_test, y_et_pred, zero_division=1)\n",
    "print(\"Single Decision Tree Classification Report:\")\n",
    "print(tree_report)\n",
    "print(\"Extra Trees Model Classification Report:\")\n",
    "print(et_report)\n",
    "\n",
    "# Visualization 1: Comparing Model Accuracies\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Single Decision Tree', 'Extra Trees'], [tree_accuracy, et_accuracy], color=['orange', 'forestgreen'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Confusion Matrix Heatmap\n",
    "conf_matrix = confusion_matrix(y_test, y_et_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Extra Trees')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# After fitting the Extra Trees model, extract the feature importances\n",
    "importances = et_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Visualization 3: Plotting the feature importances provides insight into which features the model finds most important\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "        color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and interpretation of results\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"Comparing the two models, the Extra Trees Classifier typically exhibits performance comparable to or better than Random Forest, depending on the dataset and parameter settings.\")\n",
    "print(\"The confusion matrix and classification reports for both models provide insight into their performance across different classes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f8156-9969-46e4-bd38-a0edf45d4d7e",
   "metadata": {},
   "source": [
    "# Bagging Meta-Estimator\n",
    "\n",
    "A Bagging Meta-Estimator, commonly referred to as BaggingClassifier in classification contexts or BaggingRegressor for regression tasks, embodies a robust ensemble learning strategy. By embracing the bagging (Bootstrap Aggregating) approach, it imparts the advantages of this technique to the base estimators, which can be any standard learning algorithms. This meta-estimator fits individual models on distinct random samples drawn from the initial dataset and synthesizes their predictions to arrive at a final verdict. This synthesis is accomplished through either voting or averaging, depending on the nature of the taskâ€”classification or regression respectively. The incorporation of randomness in the construction of these models serves to diminish their variance.\n",
    "\n",
    "Here's how the Bagging algorithm typically works:\n",
    "- **Step 1: Create Bootstrap Samples -** Bagging begins by creating multiple bootstrap samples from the original dataset. A bootstrap sample is a random sample of the dataset with replacement.\n",
    "- **Step 2: Train Base Estimators -** For each bootstrap sample, a base estimator (like a decision tree) is trained. Each instance of the model learns from a subset of the data.\n",
    "- **Step 3: Parallel Training -** Unlike Random Forest, which introduces randomness by selecting a subset of features at each split, Bagging uses all features for each model, and the models are trained in parallel.\n",
    "- **Step 4: Aggregation of Predictions -** Predictions for new data points are made by averaging the predictions (for regression) or by majority vote or averaging probabilities (for classification) from all individual base estimators.\n",
    "\n",
    "Here are some reasons why Bagging is powerful:\n",
    "- **Reduces Overfitting:** By averaging the results of individual estimators, it reduces the chance of overfitting.\n",
    "- - Flexibility:y:*It can be used with many different types of predictive models, not just decision trees.e.\n",
    "- Parallelizable:s:*Each model can be trained in parallel since they are independent of one another.\n",
    "- **Variance Reduction:** It is effective in reducing the variance of a prediction model, especially if the base estimator is a high variance and low bias machine learning algorithm.s.\n",
    "\n",
    "### Practical Example of Using Bagging meta-estimator\n",
    "\n",
    "In this practical session, we will explore the Bagging meta-estimator and our objectives are to:o:\n",
    "- Understand the basics of the Bagging algorithm and its strengths.s.\n",
    "- Apply Bagging to a real-world dataset and evaluate its performance.e.\n",
    "- Compare the performance of the Bagging Meta-Estimator with that of a single base estimator.\n",
    "- Visualize the stability of Bagging in terms of prediction accuracy.y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf65d1-d575-4e83-a776-deaf81afd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "# Preprocessing: encoding categorical variables, handling missing values\n",
    "# Replace missing values with NaN and then drop any rows with missing data.\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Initialize a dictionary to keep track of label encoders for each categorical column\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare data for training and testing by splitting into features and target\n",
    "X = df.drop('label', axis=1)  # Feature matrix\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets with a fixed random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a single Decision Tree Classifier for a baseline comparison\n",
    "single_tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "single_tree_clf.fit(X_train, y_train)\n",
    "y_tree_pred = single_tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_tree_pred) * 100\n",
    "\n",
    "# Initialize the Bagging Meta-Estimator with 100 Decision Trees\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging Meta-Estimator and measure the training time\n",
    "start_time = time.time()\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_train_time = time.time() - start_time\n",
    "print(f\"Bagging Meta-Estimator Training Time: {bag_train_time:.3f} seconds\")\n",
    "\n",
    "# Predict with the Bagging Meta-Estimator and measure the prediction time\n",
    "start_time = time.time()\n",
    "y_bag_pred = bag_clf.predict(X_test)\n",
    "bag_predict_time = time.time() - start_time\n",
    "print(f\"Bagging Meta-Estimator Prediction Time: {bag_predict_time:.3f} seconds\")\n",
    "\n",
    "# Evaluate the accuracy of the Bagging Meta-Estimator\n",
    "bag_accuracy = accuracy_score(y_test, y_bag_pred) *100\n",
    "\n",
    "# Print accuracy scores for both the baseline decision tree and the bagging meta-estimator\n",
    "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.2f}%\")\n",
    "print(f\"Bagging Meta-Estimator Model Accuracy: {bag_accuracy:.2f}%\")\n",
    "\n",
    "# Generate and print classification reports for both models\n",
    "tree_report = classification_report(y_test, y_tree_pred, zero_division=1)\n",
    "bag_report = classification_report(y_test, y_bag_pred, zero_division=1)\n",
    "print(\"Single Decision Tree Classification Report:\")\n",
    "print(tree_report)\n",
    "print(\"Bagging Meta-Estimator Model Classification Report:\")\n",
    "print(bag_report)\n",
    "\n",
    "# Visualization 1: Comparing Model Accuracies with a bar chart\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Single Decision Tree', 'Bagging Meta-Estimator'], [tree_accuracy, bag_accuracy], color=['orange', 'royalblue'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Confusion Matrix Heatmap for the Bagging Meta-Estimator\n",
    "# This gives us a visual understanding of the true positives, false positives, true negatives, and false negatives.\n",
    "conf_matrix_bag = confusion_matrix(y_test, y_bag_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix_bag, annot=True, fmt='d', cmap='Purples')\n",
    "plt.title('Confusion Matrix for Bagging Meta-Estimator')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and interpretation of results\n",
    "# This summarizes the findings from the comparison between a single decision tree and the bagging meta-estimator.\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"The Bagging Meta-Estimator benefits from the ensemble approach, often outperforming a single decision tree.\")\n",
    "print(\"By comparing training and prediction times as well as accuracy, one can understand the trade-offs involved in using a bagging ensemble.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6ab07-8006-4340-9901-ecb1433f0829",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3691323-9911-4721-8423-9a1b581b2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hypothetical data\n",
    "results = {\n",
    "    'Model': ['Random Forest', 'Extra-Trees', 'Bagging Meta- Estimator'],\n",
    "    'Accuracy': [rf_accuracy, et_accuracy, bag_accuracy],  \n",
    "    'Training Time': [rf_train_time, et_train_time, bag_train_time],  \n",
    "    'Prediction Time': [rf_predict_time, et_predict_time, bag_predict_time],  \n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualizing the accuracy of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Accuracy'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.yticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.ylim(0, 100)  # Assuming accuracy is between 0 and 1\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the training time of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Training Time'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Comparison of Model Training Times')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the prediction time of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Prediction Time'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Prediction Time (seconds)')\n",
    "plt.title('Comparison of Model Prediction Times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4605f5-68cd-43e3-8b64-b8b03c4ba9e0",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "When comparing Random Forest, Extra-Trees, and Bagging Meta-Estimator, consider factors beyond accuracy such as training and prediction times, and other performance characteristics.\n",
    "\n",
    "**Random Forest** is an ensemble method known for handling large datasets and providing high accuracy. Training times are relatively fast, but not as fast as simpler methods like AdaBoost. However, it offers robustness and handles complex data patterns well. It also provides feature importance metrics, aiding model interpretation.\n",
    "\n",
    "**Extra-Trees** are similar to Random Forest but introduce more randomness in building trees. This leads to faster training times as the search for optimal thresholds is not required. Extra-Trees may yield better generalization and are less sensitive to data noise but might not always be more accurate than Random Forest.\n",
    "\n",
    "Generally, **Random Forest** may be preferred for its accuracy and capability to handle imbalanced datasets automatically. **Extra-Trees** might be a better choice for faster training when some loss in precision is acceptable. **Bagging Meta-Estimator** is effective for models prone to overfitting and aiming to decrease their variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
