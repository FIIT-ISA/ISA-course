{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b7040-2a33-4d82-9d2e-892591bc3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67f292-39fe-4724-9555-8bccde957323",
   "metadata": {},
   "source": [
    "# Boosting algorithms (cca 5min running on CPU)\n",
    "\n",
    "- AdaBoost\n",
    "- Gradient tree boosting\n",
    "- eXtreme Gradient Boosting (XGBoost)\n",
    "- their comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491d8cd-bd13-4b0b-986b-2c13bb5b3027",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "AdaBoost short for Adaptive Boosting is an ensemble learning used in machine learning for classification and regression problems. The main idea behind AdaBoost is to iteratively train the weak classifier on the training dataset with each successive classifier giving more weightage to the data points that are misclassified. The final AdaBoost model is decided by combining all the weak classifier that has been used for training with the weightage given to the models according to their accuracies. The weak model which has the highest accuracy is given the highest weightage while the model which has the lowest accuracy is given a lower weightage.\n",
    "\n",
    "Here are the steps followed by the AdaBoost algorithm:\n",
    "- **Step 1: Initialize Weights -** Initially, each instance in the training dataset is assigned an equal weight of `1/N`, where `N` is the total number of instances.\n",
    "- **Step 2: Loop Over Classifiers -** AdaBoost selects a weak classifier in each iteration and adjusts its weights after each classification.\n",
    "- **Step 3: Train Weak Classifier -** A weak classifier is trained on the training data. The training process takes into account the weights of the instances.\n",
    "- **Step 4: Calculate Error -** The error rate (`err`) of the classifier is calculated as the weighted sum of the incorrectly classified instances. The error influences how much say the classifier will have in the final decision.\n",
    "- **Step 5: Compute Classifier Weight -** The classifier is assigned a weight (`alpha`) based on its accuracy. More accurate classifiers are given more weight by this formula:`alpha = 0.5 * log((1 - err) / err)`\n",
    "- **Step 6: Update Sample Weights -** Weights of the incorrectly classified instances are increased so that the classifier is forced to focus on the difficult cases in the next round. The weights of correctly classified instances are decreased. The update rules are as follows: For incorrectly classified instances: `new_weight = weight * exp(alpha)`. For correctly classified instances: `new_weight = weight * exp(-alpha)`.\n",
    "- **Step 7:  Normalize Weights -** After updating the weights, they need to be normalized so that they sum up to 1, making them a valid probability distribution.\n",
    "- **Step 8: Combine Weak Classifiers -** The weighted weak classifiers form the final model. Predictions are made by calculating the weighted majority vote of the weak classifiers.\n",
    "- **Step 9: Final Model -** The sign of the weighted sum of predictions from the weak classifiers is used to make the final prediction. A positive sum results in a positive class prediction, and a negative sum results in a negative class prediction.\n",
    "- **Step 10: Terminate or Continue -** The learning process can stop when a certain number of iterations is reached or a desired level of accuracy is achieved.\n",
    "\n",
    "### Practical Example of Using AdaBoost:\n",
    "\n",
    "In this practical session, we will explore the AdaBoost algorithm and our goals are to:\n",
    "1. Understand the basics of the AdaBoost algorithm and why it's powerful.\n",
    "2. Apply AdaBoost to a real-world dataset and observe its performance.\n",
    "3. Compare the performance of AdaBoost with a single decision tree to illustrate the benefits of using boosting.\n",
    "4. Visualize the effects of AdaBoost on decision boundaries and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658388b0-e37a-47ba-83fe-733077c5261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import seaborn as sns  # Additional import for enhanced visualizations\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "print(f\"The dataset has {df.shape[0]} instances and {df.shape[1]} features.\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Replace missing values marked as '?' with NaN and then drop these entries\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables as integers because most machine learning models require numerical input\n",
    "# Store label encoders in a dictionary for possible inverse transform if needed\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare the data for training/testing\n",
    "X = df.drop('label', axis=1)  # Features\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a DecisionTreeClassifier which will be used as the base estimator\n",
    "# with a max depth of 1, this creates a \"stump\" that AdaBoost will build upon\n",
    "base_clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Train the single Decision Tree model\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the single Decision Tree\n",
    "y_base_pred = base_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the single Decision Tree model\n",
    "base_accuracy = accuracy_score(y_test, y_base_pred) * 100\n",
    "print(f\"Base Model Accuracy: {base_accuracy:.2f}%\")\n",
    "\n",
    "# Create an AdaBoost instance with the base classifier\n",
    "# The AdaBoost model is created by combining 200 such \"stumps\" and adjusting them based on their error\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    estimator=base_clf,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Time the training of the AdaBoost model\n",
    "start_time = time.time()\n",
    "# Train the AdaBoost model\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_train_time = time.time() - start_time\n",
    "print(f\"AdaBoost Training Time: {ada_train_time:.3f} seconds\")\n",
    "\n",
    "# Time the predictions\n",
    "start_time = time.time()\n",
    "# Make predictions with AdaBoost\n",
    "y_ada_pred = ada_clf.predict(X_test)\n",
    "ada_predict_time = time.time() - start_time\n",
    "print(f\"AdaBoost Prediction Time: {ada_predict_time:.3f} seconds\")\n",
    "\n",
    "# Evaluate the AdaBoost model\n",
    "ada_accuracy = accuracy_score(y_test, y_ada_pred) * 100\n",
    "print(f\"AdaBoost Model Accuracy: {ada_accuracy:.2f}%\")\n",
    "\n",
    "# Generate and compare classification reports for each model\n",
    "# This provides a detailed breakdown of precision, recall, f1-score for each class\n",
    "base_report = classification_report(y_test, y_base_pred, zero_division=0)\n",
    "ada_report = classification_report(y_test, y_ada_pred, zero_division=0)\n",
    "print(\"Base Model Classification Report:\")\n",
    "print(base_report)\n",
    "print(\"AdaBoost Model Classification Report:\")\n",
    "print(ada_report)\n",
    "\n",
    "# Visualize the performance of the base vs. AdaBoost models\n",
    "# A horizontal bar chart is used for clear comparison between model accuracies\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Base Model', 'AdaBoost Model'], [base_accuracy, ada_accuracy], color=['skyblue', 'steelblue'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# After fitting the AdaBoost model, extract the feature importances\n",
    "importances = ada_clf.feature_importances_\n",
    "\n",
    "# Plotting the feature importances provides insight into which features the model finds most important\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(X.columns, importances, color='blue')\n",
    "plt.title(\"Feature Importances in the AdaBoost Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels to improve readability\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and Interpretation\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"From the comparison, we see that AdaBoost significantly improved the accuracy of the model compared to the base decision tree model.\")\n",
    "print(\"The feature importances show which variables have the most influence on the model's decisions, useful for further tuning or understanding the predictive factors.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffe92b-e168-4cf8-a148-5f62b94f9577",
   "metadata": {},
   "source": [
    "# Gradient tree boosting\n",
    "\n",
    "Gradient Tree Boosting is an ensemble learning technique that builds a predictive model in a stage-wise fashion. It combines the predictions from a series of weak learners, usually decision trees, to create a strong, accurate model. The algorithm focuses on minimizing a loss function by adding weak models that correct the errors made by the previous models. It's called \"gradient\" because it optimizes the model by following the gradient of the loss function.\n",
    "\n",
    "Here are the steps followed by the Gradient tree boosting algorithm:\n",
    "- **Step 1: Initialize the Model -** Start with a single tree, which could be the mean of the targets for regression, or the log odds for a classification problem.\n",
    "- **Step 2: Loop Over Trees -** Iteratively add trees to the ensemble. Each new tree is fitted on the negative gradient of the loss function (hence the name ‘Gradient’ Boosting).\n",
    "- **Step 3: Fit the Tree to Residuals -** Fit a decision tree to the negative gradient of the loss function with respect to the predictions of the current model.\n",
    "- **Step 4: Calculate the Loss -** Evaluate the loss of the current model using the loss function, which is typically mean squared error for regression and log loss for classification.\n",
    "- **Step 5: Compute Tree Output -** The output of each tree is scaled by a learning rate (also called the shrinkage factor) to prevent overfitting.\n",
    "- **Step 6: Update the Model -** Add the scaled output of the new tree to the model.\n",
    "- **Step 7: Prune the Tree -** Optionally, prune the tree to mitigate over-complexity.\n",
    "- **Step 8: Combine Trees -** The final model is the sum of the output of all trees.\n",
    "- **Step 9: Final Model -** The prediction of the ensemble is the sum of the predictions from all trees.\n",
    "- **Step 10: Terminate or Continue -** The process can be stopped when a certain number of trees is reached, or if the improvement of the loss is below a certain threshold.\n",
    "\n",
    "### Practical Example of Using Gradient Tree Boosting:\n",
    "\n",
    "In this practical session, we will delve into Gradient Tree Boosting, a powerful ensemble machine learning algorithm. Our objectives are to:\n",
    "1. Comprehend the fundamentals of Gradient Tree Boosting and its advantages.\n",
    "2. Apply Gradient Tree Boosting to a real-world dataset and examine its effectiveness.\n",
    "3. Contrast the performance of Gradient Tree Boosting with that of a single decision tree to underscore the strengths of using boosting techniques.\n",
    "4. Observe the impact of Gradient Tree Boosting on decision boundaries and the significance of features.\n",
    "\n",
    "To achieve these goals, we will proceed with the following steps in Python using `sklearn`'s `GradientBoostingClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cf556-2681-4573-9853-b150e66d3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For demonstration purposes, let's assume we're using the same dataset 'KDDCup99' as before.\n",
    "# We would load the dataset, preprocess it, and prepare the feature matrix X and labels y just like we did for AdaBoost.\n",
    "\n",
    "# Load the dataset from OpenML (same as with AdaBoost example)\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "# Preprocess the dataset: encode categorical variables, handle missing values\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare the data for training/testing\n",
    "X = df.drop('label', axis=1)  # Features\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a single Decision Tree Classifier for baseline comparison\n",
    "single_tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "single_tree_clf.fit(X_train, y_train)\n",
    "y_tree_pred = single_tree_clf.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_tree_pred) * 100\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=52, # You can change this parameter, but it will cost a lot of time to finish\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2, # You can change this parameter, but it will cost a lot of time to finish\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Time the training of the Gradient Boosting Classifier\n",
    "start_time = time.time()\n",
    "# Train the Gradient Boosting Classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_train_time = time.time() - start_time\n",
    "print(f\"Gradient Boosting Training Time: {gb_train_time:.3f} seconds\")\n",
    "\n",
    "# Time the predictions with Gradient Boosting\n",
    "start_time = time.time()\n",
    "# Make predictions with Gradient Boosting\n",
    "y_gb_pred = gb_clf.predict(X_test)\n",
    "gb_predict_time = time.time() - start_time\n",
    "print(f\"Gradient Boosting Prediction Time: {gb_predict_time:.3f} seconds\")\n",
    "\n",
    "# Evaluate the Gradient Boosting Classifier\n",
    "gb_accuracy = accuracy_score(y_test, y_gb_pred) * 100\n",
    "\n",
    "# Print out accuracy scores for both models\n",
    "print(f\"Single Decision Tree Accuracy: {tree_accuracy:.2f}%\")\n",
    "print(f\"Gradient Boosting Model Accuracy: {gb_accuracy:.2f}%\")\n",
    "\n",
    "# Generate classification reports\n",
    "tree_report = classification_report(y_test, y_tree_pred, zero_division=1)\n",
    "gb_report = classification_report(y_test, y_gb_pred, zero_division=1)\n",
    "print(\"Single Decision Tree Classification Report:\")\n",
    "print(tree_report)\n",
    "print(\"Gradient Boosting Model Classification Report:\")\n",
    "print(gb_report)\n",
    "\n",
    "# Visualize the comparison between the base decision tree and gradient boosting classifier accuracies\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Single Decision Tree', 'Gradient Boosting'], [tree_accuracy, gb_accuracy], color=['orange', 'green'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# Visualize the feature importances of the gradient boosting model\n",
    "gb_feature_importances = gb_clf.feature_importances_\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(X.columns, gb_feature_importances, color='green')\n",
    "plt.title(\"Feature Importances in Gradient Boosting Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and Interpretation\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"From the comparison, we can observe that the Gradient Boosting model has a higher accuracy than the single decision tree model.\")\n",
    "print(\"This demonstrates the effectiveness of the Gradient Boosting technique in improving the performance of weak learners by focusing on the errors of the previous trees.\")\n",
    "print(\"The visualization of feature importances provides insights into which features the model deems most important. This can inform feature engineering efforts and provide understanding of the underlying decision-making process of the model.\")\n",
    "print(\"The results underscore the utility of ensemble methods in creating robust and accurate predictive models.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4655d06-4c54-4662-bdde-8856aadf49d4",
   "metadata": {},
   "source": [
    "# eXtreme Gradient Boosting (XGBoost)\n",
    "\n",
    "Extreme Gradient Boosting, often abbreviated as XGBoost, is a powerful and highly efficient gradient boosting algorithm for both regression and classification tasks. XGBoost is an optimized and regularized version of the Gradient Boosting algorithm. It uses gradient boosting to build an ensemble of decision trees. However, XGBoost includes several innovative enhancements and regularization techniques to improve predictive accuracy, speed, and handling of large datasets.\n",
    "\n",
    "Here are the steps followed by the XGBoost algorithm:\n",
    "- **Step 1: Initialize the Model -** Begin with a base prediction which could be the average of the targets for regression, or probability of the classes for classification.\n",
    "- **Step 2: Loop Over Boosting Rounds -** Add new models to the ensemble sequentially. Each new model is trained with respect to the error of the whole ensemble learned so far.\n",
    "- **Step 3: Fit the Tree to Residuals -** Fit a decision tree to the residuals of the predictions. In XGBoost, the residuals are computed based on a chosen loss function and the predictions so far.\n",
    "- **Step 4: Calculate the Loss -** Calculate the loss using a loss function suitable for the task at hand (e.g., logistic loss for classification tasks).\n",
    "- **Step 5: Optimize Model -** XGBoost introduces the concept of regularization (L1 & L2), which can be optimized alongside the loss function to prevent overfitting.\n",
    "- **Step 6: Update the Model  -** Update the model by adding the new tree with a scaling factor called the learning rate.\n",
    "- **Step 7: Prune the Tree -** XGBoost allows for the pruning of trees, unlike traditional gradient boosting methods that grow trees to their full depth first and then prune.\n",
    "- **Step 8: Combine Trees -** The final prediction model is an ensemble of all trees built during the boosting rounds.\n",
    "- **Step 9: Cross-validation -** XGBoost allows for built-in cross-validation at each iteration of the boosting process, which allows for robust assessment of model performance.\n",
    "- **Step 10: Terminate or Continue -** The boosting process can continue until a specified number of boosting rounds are completed or until no further improvements can be made.\n",
    "\n",
    "### Practical Example of Using XGBoost:\n",
    "\n",
    "In this practical session, we are going to applying the XGBoost algorithm to a real-world dataset. The example demostrates:\n",
    "1. Comprehension of XGBoost and its operational steps.\n",
    "2. Application of XGBoost to a dataset and evaluation of its performance.\n",
    "3. Comparison of XGBoost's performance to a single decision tree.\n",
    "4. Visualization of the model's feature importances.\n",
    "\n",
    "The XGBoost library in Python is designed to provide an efficient implementation of gradient tree boosting, with additional features like regularization, which helps in preventing overfitting and can improve overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee562d7b-d795-4765-b711-3f1f27297b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time  # Import the time library\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from OpenML\n",
    "data = fetch_openml(name='KDDCup99', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "# Preprocess the dataset\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['category', 'object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare the data for training/testing\n",
    "X = df.drop('label', axis=1)  # Features\n",
    "y = df['label'].astype(int)   # Target variable\n",
    "\n",
    "# Split the dataset into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the single Decision Tree model as a base model for comparison\n",
    "base_clf = xgb.XGBClassifier(max_depth=3, n_estimators=1)  # Setting n_estimators=1 to mimic a single decision tree\n",
    "base_clf.fit(X_train, y_train)\n",
    "y_base_pred = base_clf.predict(X_test)\n",
    "base_accuracy = accuracy_score(y_test, y_base_pred) * 100\n",
    "print(f\"Base Model Accuracy (Single Decision Tree): {base_accuracy:.2f}%\")\n",
    "\n",
    "# Create an XGBoost model and measure the time\n",
    "start_time = time.time()\n",
    "xg_clf = xgb.XGBClassifier(max_depth=3, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "xg_clf.fit(X_train, y_train)\n",
    "xg_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_xg_pred = xg_clf.predict(X_test)\n",
    "xg_predict_time = time.time() - start_time\n",
    "\n",
    "xg_accuracy = accuracy_score(y_test, y_xg_pred) * 100\n",
    "print(f\"XGBoost Model Accuracy: {xg_accuracy:.2f}%\")\n",
    "print(f\"XGBoost Model Training Time: {xg_train_time:.3f} seconds\")\n",
    "print(f\"XGBoost Model Prediction Time: {xg_predict_time:.3f} seconds\")\n",
    "\n",
    "# Generate and compare classification reports\n",
    "base_report = classification_report(y_test, y_base_pred, zero_division=0)\n",
    "xg_report = classification_report(y_test, y_xg_pred, zero_division=0)\n",
    "print(\"Base Model Classification Report:\")\n",
    "print(base_report)\n",
    "print(\"XGBoost Model Classification Report:\")\n",
    "print(xg_report)\n",
    "\n",
    "# Visualize the comparison of model accuracies\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(['Base Model (Single DT)', 'XGBoost Model'], [base_accuracy, xg_accuracy], color=['orange', 'darkgreen'])\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0, 100)  # Set the x-axis limits from 0 to 100 for percentages\n",
    "plt.xticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.show()\n",
    "\n",
    "# Visualize the feature importances of the XGBoost model\n",
    "xg_feature_importances = xg_clf.feature_importances_\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(X.columns, xg_feature_importances, color='darkgreen')\n",
    "plt.title(\"Feature Importances in XGBoost Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Conclusion and Interpretation\n",
    "print(\"\\nConclusion and Interpretation:\")\n",
    "print(\"From the comparison, we observe that the XGBoost model outperforms the single decision tree model.\")\n",
    "print(\"The feature importances plot reveals which features the model considers most predictive for the given task.\")\n",
    "print(\"XGBoost's model accuracy and feature importance analysis provide valuable insights for model tuning and interpretation.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090da39e-7958-477c-bbac-3ea4f41e31f7",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447be4f-4d9e-49f1-853c-5736e981238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hypothetical data\n",
    "results = {\n",
    "    'Model': ['AdaBoost', 'Gradient Boosting', 'XGBoost'],\n",
    "    'Accuracy': [ada_accuracy, gb_accuracy, xg_accuracy],  \n",
    "    'Training Time': [ada_train_time, gb_train_time, xg_train_time],  \n",
    "    'Prediction Time': [ada_predict_time, gb_predict_time, xg_predict_time],  \n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualizing the accuracy of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Accuracy'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.yticks(np.arange(0, 101, 10))  # Set the ticks to be in increments of 10%\n",
    "plt.ylim(0, 100)  # Assuming accuracy is between 0 and 1\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the training time of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Training Time'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Comparison of Model Training Times')\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the prediction time of each model\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(results_df['Model'], results_df['Prediction Time'], color=['blue', 'green', 'red'])\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Prediction Time (seconds)')\n",
    "plt.title('Comparison of Model Prediction Times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa8a2a-6a7d-4ad4-8ee9-d178ca4a8362",
   "metadata": {},
   "source": [
    "## Final notes:\n",
    "When comparing AdaBoost, Gradient Boosting, and XGBoost, there are several factors to consider beyond just accuracy, such as training time and prediction time.\n",
    "\n",
    "**AdaBoost** is often faster to train than the other two boosting methods, making it a good choice for quick prototyping or when working with smaller datasets. However, it may not perform as well with very noisy data or complex problems where the decision boundaries are not as straightforward.\n",
    "\n",
    "**Gradient Boosting** is a more robust technique compared to AdaBoost and tends to perform better on a wider range of problems, including those with complex decision boundaries and heterogeneous features. It is slower than AdaBoost due to the sequential nature of boosting the weak learners, but it often results in better performance.\n",
    "\n",
    "**XGBoost** is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. XGBoost provides a parallel tree boosting that solves many data science problems in a fast and accurate way. For large datasets, or when you are seeking the highest accuracy, XGBoost is often the preferred algorithm due to its efficiency and the capability of handling sparse data.\n",
    "\n",
    "In summary, **AdaBoost** can be preferred for its simplicity and speed, **Gradient Boosting** for its robustness, and **XGBoost** for its performance and scalability. The choice of algorithm ultimately depends on the specific problem, the size and nature of the data, the computational resources available, and the balance between training time and model performance that you are willing to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
