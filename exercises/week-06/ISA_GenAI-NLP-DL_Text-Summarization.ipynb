{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44231a1a-ff56-43bb-93cc-d5600b24ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686b57d-f2ae-4c0c-872e-e630008d1749",
   "metadata": {},
   "source": [
    "### Install libraries\n",
    "\n",
    "**IMPORTANT:** Currently BERT in this notebook does not works well with Mac M1 and M2 hardware architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e27b6a-9156-49d6-8b6f-a3a6e338d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3b5b54381fcdb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Text summarization \n",
    "\n",
    "**Sources:**\n",
    "- https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25\n",
    "- https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "- https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/\n",
    "- https://iq.opengenus.org/bert-for-text-summarization/\n",
    "\n",
    "Text summarization refers to the technique of condensing a lengthy text document into a short and well-written summary that captures the essential information andmainideas of the original text.\n",
    "This process is achieved by highlighting the significant points of the document.\n",
    "\n",
    "There are **two different approaches** used for text summarization:\n",
    "- Extractive Summarization\n",
    "- Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62031f1a-6907-4acf-ae50-53b619f90ff4",
   "metadata": {},
   "source": [
    "# Extractive Summarization\n",
    "\n",
    "In extraction-based summarization, a subset of words that represent the most important points is pulled from a piece of text and combined to make a summary.\n",
    "In machine learning, extractive summarization usually involves weighing the essential sections of sentences and using the results to generate summaries.\n",
    "\n",
    "Summarizing the text consists of the following steps: \n",
    "\n",
    "1. **Preprocessing:**\n",
    "\n",
    "    Tokenization: Break the text into individual words or phrases (Simple White-Space Tokenization, Regular Expression-Based Tokenization). \n",
    "\n",
    "    Stop Words Removal: Eliminate common words that do not carry significant meaning (libraries such as NLTK and Spacy).\n",
    "\n",
    "    Lemmatization or Stemming: Reduce words to their base form to normalize the text (Porter stemming algorithm, Lancaster stemming algorithm,lemmatization techniques provided by libraries like NLTK or Spacy).\n",
    "\n",
    "2. **Sentence Scoring:**\n",
    "\n",
    "    Sentence Importance Calculation: Assign scores to sentences based on different features such as word frequency, sentence length, and position in the document.\n",
    "\n",
    "    Use of Statistical Methods: Apply statistical techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to measure the importance of each word in the context of the entire\n",
    "\n",
    "3. **Sentence Ranking:**\n",
    "\n",
    "    Rank sentences based on their calculated importance scores.\n",
    "\n",
    "    Identify the top-ranked sentences as potential candidates for the summary.\n",
    "\n",
    "4. **Summary Generation:**\n",
    "\n",
    "    Select Top Sentences: Choose the top-ranked sentences based on the predetermined summary length or the desired compression ratio.\n",
    "\n",
    "    Arrange the Selected Sentences: Organize the selected sentences in a coherent manner to ensure the flow and coherence of the summary.\n",
    "\n",
    "    Optional Post-Processing: Perform additional linguistic processing to improve the grammatical structure and overall readability of the summary.\n",
    "\n",
    "5. **Output:**\n",
    "\n",
    "    Generate the final extractive summary by combining the selected sentences.\n",
    "\n",
    "    Present the summary in a readable format that effectively captures the key points of the source text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acd4d4-7741-4857-9aaa-e917982c08d7",
   "metadata": {},
   "source": [
    "### The example of using extractive text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T22:19:51.733980400Z",
     "start_time": "2024-03-08T22:19:44.942952100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest\n",
    "\n",
    "# Load the SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "\n",
    "    # Phase 1: Preprocessing\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Build word frequency\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text not in STOP_WORDS:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "    # Get the most frequent word\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "\n",
    "    # Normalize the frequencies\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    # Phase 2: Sentence Scoring\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    # Phase 3: Sentence Ranking\n",
    "    summarized_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    final_sentences = [w.text for w in summarized_sentences]\n",
    "\n",
    "    # Phase 4: Summary Generation\n",
    "    summary = ' '.join(final_sentences)\n",
    "    return summary\n",
    "\n",
    "def read_text_from_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Read input text from file\n",
    "file_name = 'text_summarization.txt'\n",
    "text = read_text_from_file(file_name)\n",
    "\n",
    "# Generate and print the summary\n",
    "summary = summarize_text(text)\n",
    "\n",
    "# Phase 5: Output \n",
    "print(\"Text summarization:\\n\\n\" + summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03327e6bf296079",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Abstractive Summarization using pretrained model(s) - BERT\n",
    "\n",
    "In abstractive summarization, advanced deep learning techniques are applied to paraphrase and shorten the original document, just like humans do.\n",
    "Since abstractive machine learning algorithms can generate new phrases and sentences that represent the most important information from the source text, they can assist in overcoming the grammatical inaccuracies of the extraction techniques. \n",
    "\n",
    "### The example of using abstractive text summarization\n",
    "\n",
    "In the method of abstractive text summarization, we will use the pretrained model BERT.\n",
    "\n",
    "Utilizing BERT for text summarization involves fine-tuning the pre-trained model on a dataset specific to summarization tasks. This process leverages BERT's extensive knowledge base, acquired from pre-training on a vast corpus of text, to adapt its capabilities to the specific requirements of summarization. The result is a powerful tool that can efficiently process large documents, extract key points, and present them in a clear, concise manner, transforming the way information is consumed and comprehended in the digital age.\n",
    "\n",
    "The \"Bidirectional\" part of BERT means that this assistant doesnâ€™t just look at the words before or after a given word to understand its meaning; it considers the entire sentence, or even multiple sentences, at once. This comprehensive view allows it to grasp the subtleties of language, such as how the meaning of a word can change based on the words around it.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "1. Contextual Understanding: BERT's bidirectional nature allows it to understand the context of words in a sentence more effectively than many previous models, leading to more accurate and coherent summaries.\n",
    "2. Pre-trained Model: Since BERT has been pre-trained on a vast corpus of text, it comes with a general understanding of language, which can significantly reduce the time and resources required for model training for specific summarization tasks.\n",
    "3. Versatility: BERT can be fine-tuned with additional layers for a wide range of NLP tasks beyond summarization, such as question answering and sentiment analysis, making it a versatile tool in the NLP toolkit.\n",
    "4. High Performance: BERT has demonstrated state-of-the-art performance on numerous NLP benchmarks, indicating its capability to produce high-quality text summaries.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "1. Resource Intensive: BERT's complexity and the size of its neural network make it computationally expensive, requiring significant hardware resources for training and inference, which might not be accessible to everyone.\n",
    "2. Fine-tuning Challenges: While BERT can be fine-tuned for specific tasks, the process requires NLP expertise and can be time-consuming to optimize for best performance on text summarization specifically.\n",
    "3. Overfitting Risk: Given its large parameter count, there's a risk of overfitting, especially when fine-tuning on smaller datasets. This could lead to less generalizable models that don't perform well on unseen data.\n",
    "4. Handling of Long Documents: BERT has a maximum token limit (typically 512 tokens), which can be a limitation for summarizing longer documents directly, necessitating workarounds that may complicate the summarization process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a7ff9-e4d4-435d-97c6-cafe18dd9808",
   "metadata": {},
   "source": [
    "### Running note\n",
    "- the following code downloads more than 1.34 GB model and metadata\n",
    "- inference may take longer time (several minutes) based on hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287856cf38e11f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T22:19:58.343298600Z",
     "start_time": "2024-03-08T22:19:52.660167800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress FutureWarnings, specifically those from sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# Initialize the model\n",
    "model=Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d235772fd65b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T22:22:37.956392400Z",
     "start_time": "2024-03-08T22:19:58.353559100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read input text from file\n",
    "file_name = 'text_summarization.txt'\n",
    "text = read_text_from_file(file_name)\n",
    "\n",
    "summary=model(text)\n",
    "print(\"Text summarization:\\n\\n\" + summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
