{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layers \n",
    "\n",
    "- Based on https://www.kaggle.com/code/colinmorris/embedding-layers/notebook\n",
    "- Dataset: [Decreased preprocessed MovieLens dataset](https://www.kaggle.com/code/colinmorris/movielens-preprocessing/output?select=mini_rating.csv)\n",
    "\n",
    "### The goal \n",
    "to explain embedding layers from both theoretical and practical side. We'll learn when and how to use them. As a result we will train a simple model predicting which rating will the user give to a film basing on their previous ratings.\n",
    "\n",
    "### Sparse Categorical Variables\n",
    "Embeddings are a technique that enable deep neural nets to work with **sparse categorical variables**.\n",
    "By this we mean a categorical variable with lots of possible values (high *cardinality*), with a small number of them (often just 1) present in any given observation. One good example is words. There are hundreds of thousands of them in the English language, but a single tweet might only have a dozen. Word embeddings are a crucial technique for applying deep learning to natural language. But other examples abound.\n",
    "\n",
    "For example, [this dataset of LA county restaurant inspections](https://www.kaggle.com/meganrisdal/la-county-restaurant-inspections-and-violations) has several sparse categorical variables, including:\n",
    "- `employee_id`: which of the health department's employees performed this inspection? (~250 distinct values)\n",
    "- `facility_zip`: what zip code is the restaurant located in? (~3,000 distinct values)\n",
    "- `owner_name`: who owns the restaurant? (~35,000 distinct values)\n",
    "\n",
    "An embedding layer would be a good idea for using any of these variables as inputs to a network.\n",
    "\n",
    "### MovieLens dataset\n",
    "The [MovieLens](https://www.kaggle.com/code/colinmorris/movielens-preprocessing/output?select=mini_rating.cs) dataset consists of ratings assigned to movies by users. Here's a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0256299053774ce4384d6c5b422ee9d196bb3f21"
   },
   "outputs": [],
   "source": [
    "# Setup. Import libraries and load dataframes for Movielens data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(1); # Set random seeds for reproducibility\n",
    "np.random.seed(1); \n",
    "random.seed(1) \n",
    "\n",
    "input_dir = './data'\n",
    "ratings_path = os.path.join(input_dir, 'mini_rating.csv')\n",
    "\n",
    "ratings_df = pd.read_csv(ratings_path, usecols=['userId', 'movieId', 'rating', 'y'])\n",
    "movies_df = pd.read_csv(os.path.join(input_dir, 'movie.csv'), usecols=['movieId', 'title', 'year'])\n",
    "\n",
    "df = ratings_df.merge(movies_df, on='movieId').sort_values(by='userId')\n",
    "df = df.sample(frac=1, random_state=1) # Shuffle\n",
    "\n",
    "df.sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5239d74ad582b777918aac9dc8d81907fcba44da"
   },
   "source": [
    "Ratings range from 0.5 stars to 5. Our goal will be to predict the rating a given user $u_i$ will give a particular movie $m_j$. (The column `y` is just a copy of the rating column with the mean subtracted - this will be useful later.)\n",
    "\n",
    "`userId` and `movieId` are both sparse categorical variables. They have many possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6a1605bc1d7af9b9b40f405bf08711e3c033c334"
   },
   "outputs": [],
   "source": [
    "n_movies = len(df.movieId.unique())\n",
    "n_users = len(df.userId.unique())\n",
    "print(\n",
    "    \"{1:,} distinct users rated {0:,} different movies (total ratings = {2:,})\".format(\n",
    "        n_movies, n_users, len(df),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccc058ea5cda9e6264c73cef4bf07e872d229957"
   },
   "source": [
    "# Building a rating prediction model in Keras\n",
    "\n",
    "We want to build a model that takes a user, $u_i$ and a movie, $m_j$, and outputs a number from 0.5-5, representing how many stars we think this user would give that movie. \n",
    "\n",
    "> **Aside:** You may have noticed that the [MovieLens dataset](https://www.kaggle.com/grouplens/movielens-20m-dataset) includes information about each movie such as its title, its year of release, a set of genres and user-assigned tags. But for now, we're not going to try to exploit any of that extra information.\n",
    "\n",
    "We claim we need an embedding layer to handle these inputs. Why? Let's review some alternatives and see why they don't work.\n",
    "\n",
    "### Bad idea #1: Use user ids and movie ids as numerical inputs\n",
    "\n",
    "Why not feed in user ids and movie ids as inputs, then add on some dense layers and call it a day? i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "051cb8c39117da3f9b5a50e45181d066a0b9904b"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # 2 input values: user id and movie id\n",
    "    keras.layers.Dense(256, input_dim=2, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    # A single output node, containing the predicted rating\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd4d97ae9c78c818f6570f69ce598ea4206349c0"
   },
   "source": [
    "In the simplest terms, neural nets work by doing math on their inputs. But the actual numerical values of the ids assigned to users and movies are meaningless. *Schindler's List* has id 527 and *The Usual Suspects* has id 50, but that doesn't mean *Schindler's List* is 'ten times bigger' than *The Usual Suspects*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "267ebc871b52db908f57dea980e11a0a144ee014"
   },
   "source": [
    "### Bad idea #2: One-hot encoded user and movie inputs\n",
    "\n",
    "One-hot encoding is \"The Standard Approach for Categorical Data\". So why is it a bad idea here? Let's see what a model would look like that took one-hot encoded users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f40fde6725d83c995b17eeb2c84262c14b22a421"
   },
   "outputs": [],
   "source": [
    "input_size = n_movies + n_users\n",
    "print(\"Input size = {:,} ({:,} movies + {:,} users)\".format(\n",
    "    input_size, n_movies, n_users,\n",
    "))\n",
    "model = keras.Sequential([\n",
    "    # One hidden layer with 128 units\n",
    "    keras.layers.Dense(128, input_dim=input_size, activation='relu'),\n",
    "    # A single output node, containing the predicted rating\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02e7c6667f9ead3894a015a06f5e59ee547060ad"
   },
   "source": [
    "A basic issue here is scaling and efficiency. As we can see, a single input to our model is a vector of 18,751 numbers (of which we know that 18,749 will be zeros). In this case we only used a reduced dataset, the feature data for whole MovieLens dataset of 20 million rating instances will require a 2-d array of size 20,000,000 x 165,237, or about 3 **trillion** numbers. Good luck fitting that all into memory at once!\n",
    "\n",
    "Also, doing training and inference on such model will be inefficient. To calculate the activations of first hidden layer, we'll need to multiply 165k inputs through about 21 million weights - but the vast, vast majority of those products will just be zero.\n",
    "\n",
    "One-hot encoding is fine for categorical variables with a small number of possible values, like `{Red, Yellow, Green}`, or `{Monday, Tuesday, Wednesday, Friday, Saturday, Sunday}`. But it's not so great in cases like our movie recommendation problem, where variables have tens or hundreds of thousands of possible values.\n",
    "\n",
    "## Good idea: Embedding layers\n",
    "\n",
    "In short, an **embedding layer** maps each element in a set of discrete things (like words, users, or movies) to a dense vector of real numbers (its **embedding**). \n",
    "\n",
    "> **Aside:** A key implementation detail is that embedding layers take as input the *index* of the entity being embedded (i.e. we can give it our userIds and movieIds as input). You can think of it as a sort of 'lookup table'. This is much more efficient than taking a one-hot vector and doing a huge matrix multiplication!\n",
    "\n",
    "As an example, if we learn embeddings of size 8 for movies, the embedding for *Legally Blonde* (index=4352) might look like:\n",
    "\n",
    "$$[ 1.624, -0.612, -0.528, -1.073,  0.865, -2.302,  1.745, -0.761]$$\n",
    "\n",
    "**Where do these come from?** We initialize an embedding for each user and movie using random noise, then we train them as part of the process of training the overall rating-prediction model. \n",
    "\n",
    "**What do they mean?** An object's embedding, if it's any good, should capture some useful latent properties of that object. But the key word here is *latent* AKA hidden. It's up to the model to discover whatever properties of the entities are useful for the prediction task, and encode them in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa70585e512ee6e095d5309034d344c74fb7d533"
   },
   "source": [
    "## Implementing it: we want our model to look something like this:\n",
    "\n",
    "<img src=\"https://i.imgur.com/Z1eVQu9.png\" alt=\"embedding\" width=\"600\"/>\n",
    "<!-- ![Imgur](https://i.imgur.com/Z1eVQu9.png) //-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cc9049b23066e18046fca655b39ad601158af85"
   },
   "source": [
    "A key thing to note is that this network is not simply a stack of layers from input to output. We're treating the user and the movie as separate inputs, which come together only after each has gone through its own embedding layer.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a1cc608e9f334d2a356cc5b00216baa50a52fcb"
   },
   "outputs": [],
   "source": [
    "hidden_units = (32,4)\n",
    "movie_embedding_size = 8\n",
    "user_embedding_size = 8\n",
    "\n",
    "# Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "\n",
    "# Concatenate the embeddings (and remove the useless extra dimension)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model.summary(line_length=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70da17016222cdbbb9d7705522cbfd33bb316c70"
   },
   "source": [
    "## Training it\n",
    "\n",
    "We'll compile our model to minimize squared error ('MSE'). We'll also include absolute error ('MAE') as a metric to report during training, since it's a bit easier to interpret.\n",
    "\n",
    "> Something to think about: We know that ratings can only take on the values `{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}` - so why not treat this as a multiclass classification problem with 10 classes, one for each possible star rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39751565e9338ec5197e3fd0afce6fcef7dac1ea"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    tf.optimizers.Adam(0.005),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56cdd49ec046ddf40d828c594cc8b88f3a643398"
   },
   "source": [
    "Let's train the model.\n",
    "\n",
    "> **Aside**: We are passing in `df.y` as target variable rather than `df.rating`. The `y` column is just a 'centered' version of the rating - i.e. the rating column minus its mean over the training set. For example, if the overall average rating in the training set was 3 stars, then we would translate 3 star ratings to 0, 5 star ratings to 2.0, etc. to get `y`. This is a common practice in deep learning, and tends to help achieve better results in fewer epochs. For more details, feel free to check out [this kernel](https://www.kaggle.com/colinmorris/movielens-preprocessing) with all the preprocessing on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4949348044a9307736945766e24b732bf8ffdb6"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [df.userId, df.movieId],\n",
    "    df.y,\n",
    "    batch_size=5000,\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    validation_split=.05,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b5fbd39513e8aa02318ab09f891222a65438435"
   },
   "source": [
    "To judge whether our model is any good, it'd be helpful to have a baseline. In the cell below, we calculate the error of a couple dumb baselines: always predicting the global average rating, and predicting the average rating per movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "24e1226b8ded8b9e95b880ab36083f31bd5cb2e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=.05, random_state=1)\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return metrics.mean_absolute_error(y_true, y_pred), metrics.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "mean_rating = df_train['rating'].mean()\n",
    "print(\"Average rating in training set is {:.2f} stars\".format(mean_rating))\n",
    "\n",
    "y_true = df_val['rating'].values\n",
    "always_mean = np.full(y_true.shape, mean_rating)\n",
    "\n",
    "mae, mse = get_metrics(y_true, always_mean)\n",
    "print(\"Always predicting global average rating results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(\n",
    "    mae, mse))\n",
    "\n",
    "movies = movies_df.copy().set_index('movieId')\n",
    "mean_per_movie = df_train.groupby('movieId')['rating'].mean()\n",
    "movies['mean_rating'] = mean_per_movie\n",
    "ratings_per_movie = df_train.groupby('movieId').size()\n",
    "movies['n_ratings'] = ratings_per_movie\n",
    "# There are a few movies in the validation set not present in the training set. We'll just use the global\n",
    "# mean rating in their case.\n",
    "y_movie_mean = df_val.join(mean_per_movie, on='movieId', rsuffix='mean')['ratingmean'].fillna(mean_rating).values\n",
    "\n",
    "mae, mse = get_metrics(y_true, y_movie_mean)\n",
    "print(\"Predicting mean per movie results in Mean Absolute Error={:.2f}, Mean Squared Error={:.2f}\".format(mae, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6537eaa40ec56101f79527cc58f516bdf3504382"
   },
   "source": [
    "Here's a plot of our embedding model's absolute error over time. For comparison, our best baseline (predicting the average rating per movie) is marked with a dotted line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "250a4ecbf2b274ed6ba99c781a4bba962eb7167c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(history.epoch, history.history['val_MAE'], label='Validation MAE')\n",
    "ax.plot(history.epoch, history.history['MAE'], label='Training MAE')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_xlim(left=0, right=history.epoch[-1])\n",
    "baseline_mae = 0.73\n",
    "ax.axhline(baseline_mae, ls='--', label='Baseline', color='#002255', alpha=.5)\n",
    "ax.grid()\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f5eef91cbeaf72c3e39e8dd66d00cd2b68449b7"
   },
   "source": [
    "Compared to the baseline, we were able to get our average error down by more than .1 stars (or about 15%). Not bad!\n",
    "\n",
    "## Example predictions\n",
    "\n",
    "Let's try some example predictions as a sanity check. We'll start by picking out a specific user from the dataset at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9a4e4a1c144b8fbfd2aac2516ae6238f6cc2952f"
   },
   "outputs": [],
   "source": [
    "ratings_per_user = df.groupby('userId').size()\n",
    "uid = ratings_per_user[ratings_per_user < 30].sample(1, random_state=1).index[0]\n",
    "user_ratings = df[df.userId==uid]\n",
    "\n",
    "print(\"User #{} has rated {} movies (avg. rating = {:.1f}):\".format(\n",
    "    uid, len(user_ratings), user_ratings['rating'].mean(),\n",
    "))\n",
    "\n",
    "cols = ['userId', 'movieId', 'rating', 'title', 'year']\n",
    "user_ratings.sort_values(by='rating', ascending=False)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b28e7bf92d12f4633901035f71f0451b95d0200d"
   },
   "outputs": [],
   "source": [
    "candidate_movies = movies[:5].copy()\n",
    "\n",
    "preds = model.predict([\n",
    "    np.array([uid] * len(candidate_movies)), # User ids \n",
    "    candidate_movies.index.values, # Movie ids\n",
    "])\n",
    "\n",
    "# NB: Remember we trained on 'y', which was a version of the rating column centered on 0. To translate\n",
    "# our model's output values to the original [0.5, 5] star rating scale, we need to 'uncenter' the\n",
    "# values, by adding the mean back\n",
    "\n",
    "row = df.iloc[0] # The difference between rating and y will be the same for all rows, so we can just use the first\n",
    "y_delta = row.rating - row.y\n",
    "candidate_movies['predicted_rating'] = preds + y_delta\n",
    "\n",
    "# Add a column with the difference between our predicted rating (for this user) and the movie's\n",
    "# overall average rating across all users in the dataset.\n",
    "\n",
    "candidate_movies['delta'] = candidate_movies['predicted_rating'] - candidate_movies['mean_rating']\n",
    "candidate_movies.sort_values(by='delta', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "learntools_metadata": {
   "lesson_index": 0,
   "type": "tutorial"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
