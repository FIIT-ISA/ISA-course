{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9152a",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Data Mining (PPDM)\n",
    "\n",
    "Privacy-Preserving Data Mining (PPDM) is a research area focused on developing methods and techniques to protect the privacy of individuals when mining sensitive data. The goal is to extract valuable information from large datasets while ensuring that the privacy of individuals represented in the data is not compromised.\n",
    "PPDM is comprised of several key components and techniques that work together to protect privacy.\n",
    "\n",
    "# I. Data Transformation Techniques\n",
    "\n",
    "## 1. Data Anonymization\n",
    "\n",
    "This involves altering the dataset in such a way that the identity of individuals cannot be easily linked with their corresponding data records. There are several techniques for data anonymization, including:\n",
    "\n",
    "- **K-Anonymity:** This approach ensures that each individual is indistinguishable from at least \\( k-1 \\) other individuals within the dataset. This is achieved by generalizing and suppressing certain identifiers.\n",
    "- **L-Diversity:** An extension of k-anonymity, l-diversity requires that each \"equivalence class\" (a group of records that are indistinguishable from each other) has at least \\( l \\) \"well-represented\" values for each sensitive attribute. This helps to prevent attribute disclosure.\n",
    "- **T-Closeness:** This further extends l-diversity by ensuring that the distribution of a sensitive attribute in each equivalence class is close to the distribution of the attribute in the entire table, maintaining a \"t\" level of closeness.\n",
    "\n",
    "## 2. Data Perturbation\n",
    "\n",
    "Data perturbation involves adding noise to the data before it is published or analyzed. This can be done in a variety of ways, such as by swapping values between records (data swapping), adding random noise to values (additive noise), or using other statistical techniques to obscure the original data.\n",
    "\n",
    "## 3. Data Swapping\n",
    "\n",
    "This technique swaps data values among similar records, aiming to preserve the overall statistical properties of the data while protecting the privacy of individual records.\n",
    "\n",
    "\n",
    "## 4. Differential Privacy\n",
    "\n",
    "Differential privacy provides a mathematical framework for quantifying privacy. It offers a strong privacy guarantee that the output of a database query is not significantly altered by the presence or absence of a single individual's data. This is achieved by adding a certain amount of noise to the results of queries, which is carefully calibrated to maintain utility while protecting privacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c5e50",
   "metadata": {},
   "source": [
    "# II. Data Security Techniques\n",
    "\n",
    "## 1. Encryption\n",
    "\n",
    "This cryptographic method allows computations to be performed on encrypted data without needing to decrypt it first. The results of such computations are also encrypted and can only be decrypted by the owner of the private key. This ensures that sensitive data can be processed without compromising privacy.\n",
    "\n",
    "\n",
    "## 2. Homomorphic Encryption (HE)\n",
    "\n",
    "Homomorphic encryption (HE) is a form of encryption that allows computations to be carried out on ciphertext, generating an encrypted result which, when decrypted, matches the result of operations performed on the plaintext. This is a powerful tool for privacy-preserving data mining (PPDM) as it allows data to be encrypted and still be useful for computations without exposing the original data.\n",
    "\n",
    "#### Types of Homomorphic Encryption:\n",
    "\n",
    "1. **Partially Homomorphic Encryption (PHE)**: Supports unlimited operations of either addition or multiplication, but not both. An example is the RSA algorithm, which is homomorphic over multiplication.\n",
    "\n",
    "2. **Somewhat Homomorphic Encryption (SHE)**: Allows both additions and multiplications, but only a limited number of operations in total. The limit comes from the noise that accumulates with each operation, which eventually makes the ciphertext too noisy to be decrypted correctly.\n",
    "\n",
    "3. **Fully Homomorphic Encryption (FHE)**: Supports unlimited additions and multiplications without any constraints on the number of operations. This is the most flexible form of homomorphic encryption but is also the most computationally intensive. The first FHE scheme was proposed by Craig Gentry in 2009.\n",
    "\n",
    "4. **Leveled Homomorphic Encryption (LHE)**: A variant of FHE that supports a limited number of multiplications, which must be defined in advance. This limits the depth of the arithmetic circuits that can be evaluated but reduces the computational overhead compared to full FHE.\n",
    "\n",
    "#### Homomorphic Encryption in PPDM:\n",
    "\n",
    "Homomorphic encryption can be used in PPDM to:\n",
    "\n",
    "- Securely outsource computations to a cloud environment.\n",
    "- Perform privacy-preserving statistical analysis.\n",
    "- Enable secure multi-party computation where different parties can jointly compute a function over their inputs while keeping those inputs private.\n",
    "\n",
    "However, the computational complexity of homomorphic encryption has historically made it impractical for large-scale or real-time applications. Recent advances have improved its efficiency, but it remains more resource-intensive than other encryption methods.\n",
    "\n",
    "#### Applying homomorphic encryption typically involves several steps:\n",
    "\n",
    "1. Key Generation: Generate public and private keys.\n",
    "2. Encryption: Encrypt the data using the public key.\n",
    "3. Computation: Perform encrypted computations.\n",
    "4. Decryption: Decrypt the result using the private key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ac989",
   "metadata": {},
   "source": [
    "# III. Distributed Learning\n",
    "\n",
    "## 1. Secure Multi-Party Computation (SMPC)\n",
    "\n",
    "Secure Multi-Party Computation (SMPC) is a cryptographic protocol that enables multiple parties to jointly compute a function over their inputs while keeping those inputs private. The parties learn the result of the function but nothing else about each other's inputs. This is particularly useful when you have datasets that cannot be combined due to privacy concerns or regulatory restrictions.\n",
    "\n",
    "#### How SMPC Works:\n",
    "\n",
    "1. **Sharing**: Each party's input is split into \"shares\" that are distributed to all participants. The shares are created in such a way that only a certain subset (or all) of them combined can reconstruct the original input.\n",
    "\n",
    "2. **Computation**: Parties perform computations on the shares. The computations are designed such that, when the shares of the result are combined, they yield the result of the computation on the original inputs.\n",
    "\n",
    "3. **Reconstruction**: After computation, the shares of the result are combined to reconstruct the final output.\n",
    "\n",
    "#### Properties of SMPC:\n",
    "\n",
    "- **Privacy**: Even if a party is compromised, the attackers cannot learn anything about the inputs as long as a threshold number of parties remain uncompromised.\n",
    "- **Correctness**: The result of the computation is correct if all parties follow the protocol.\n",
    "- **Fault-tolerance**: Some SMPC protocols can tolerate a certain number of parties being offline or acting maliciously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe826f",
   "metadata": {},
   "source": [
    "## 2. Federated Learning (FL)\n",
    " \n",
    "Federated learning is a machine learning approach that enables model training on a large corpus of decentralized data. It's a privacy-preserving technique where the training data stays on the users' device, and only model updates (parameters or gradients) are sent to a central server. This means that the raw data never leaves its original location, which is beneficial for privacy. \n",
    "\n",
    "#### Step-by-Step Explanation of Federated Learning:\n",
    "\n",
    "1. **Initialization**: A global model is initialized on a central server.\n",
    "\n",
    "2. **Distribution**: The model is sent to multiple participants (e.g., devices or organizations) which have local data.\n",
    "\n",
    "3. **Local Training**: Each participant trains the model on their local data to create a local model update. No raw data is shared.\n",
    "\n",
    "4. **Uploading**: The model updates (e.g., weights, gradients) are sent from the participants to the central server. Communication is often encrypted to enhance privacy.\n",
    "\n",
    "5. **Aggregation**: The central server aggregates these updates to improve the global model. A common method for this is Federated Averaging.\n",
    "\n",
    "6. **Update**: The updated global model is then sent back to the participants.\n",
    "\n",
    "7. **Iteration**: Steps 2-6 are repeated until the model performance reaches a satisfactory level.\n",
    "\n",
    "\n",
    "Federated learning represents a significant shift in how AI systems are trained, focusing on privacy and decentralized data. It's particularly relevant in the context of increasing data privacy regulations and growing public awareness about data security.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b957ef",
   "metadata": {},
   "source": [
    "# VI. Federated Learning: Real-World Example - Google's Gboard\n",
    "\n",
    "One of the most cited examples of federated learning in action is Google's Gboard, the keyboard application used on Android devices. When Google wanted to improve the predictive text feature without uploading sensitive user data to their servers, they turned to federated learning.\n",
    "\n",
    "- **Initialization**: Google creates a base predictive model.\n",
    "\n",
    "- **Distribution**: The model is distributed to users through an app update.\n",
    "\n",
    "- **Local Training**: As users type, the local model on their phone learns from the input and gets better at predicting text.\n",
    "\n",
    "- **Uploading**: Periodically, the phone uploads the model improvements (not the text typed) to Google's server.\n",
    "\n",
    "- **Aggregation**: Google combines these updates from millions of users to improve the global predictive model.\n",
    "\n",
    "- **Update**: This improved model is then pushed to users in a subsequent update.\n",
    "\n",
    "- **Iteration**: This process continues, constantly improving the model's predictions.\n",
    "\n",
    "#### Benefits:\n",
    "\n",
    "- **Privacy**: Users' sensitive data, like personal messages, are never directly exposed to the server.\n",
    "- **Efficiency**: Reduces the need to transmit large datasets; only model updates are communicated.\n",
    "- **Scalability**: Can handle a large number of participants and data points.\n",
    "\n",
    "#### Challenges:\n",
    "\n",
    "- **Communication Overhead**: Requires many rounds of communication, which can be costly and slow, especially if participants have poor internet connections.\n",
    "- **System Heterogeneity**: Participants may have devices with varying computational and battery capacities.\n",
    "- **Data Heterogeneity**: Data distribution may not be identical across devices, leading to skewed models if not handled properly.\n",
    "\n",
    "#### Applications in Known Products/Brands:\n",
    "\n",
    "Apart from Google's Gboard, other applications include:\n",
    "\n",
    "- **Apple**: Uses differential privacy and federated learning to collect data and improve Siri and QuickType suggestions.\n",
    "- **Healthcare**: Companies like Owkin use federated learning to develop predictive models without sharing patient data.\n",
    "- **Finance**: Banks and financial institutions use federated learning to detect fraud and analyze risk without centralizing sensitive financial data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78246549",
   "metadata": {},
   "source": [
    "# V. Use cases with PPDM techniques:\n",
    "\n",
    "1. **Healthcare Data**: Medical records, clinical trial data, and other patient-related information that contain personal health information.\n",
    "\n",
    "2. **Financial Data**: Banking transactions, credit card transactions, and financial records that include personal financial information.\n",
    "\n",
    "3. **Educational Data**: Student records, grades, and other personal information related to educational institutions.\n",
    "\n",
    "4. **Retail Data**: Customer purchase histories, loyalty program data, and other data that can reveal personal buying habits and preferences.\n",
    "\n",
    "5. **Telecommunications Data**: Call detail records, location data, and usage patterns that are sensitive in nature.\n",
    "\n",
    "6. **Social Networking Data**: Personal interactions, connections, posts, and messages that are often private.\n",
    "\n",
    "7. **Location Data**: Data from GPS devices, mobile apps, and other sources that can track an individual's movements.\n",
    "\n",
    "8. **Employment Data**: Employee records, performance data, and other personal information held by employers.\n",
    "\n",
    "9. **Government Records**: Personal information in government databases, such as tax records, social security data, and census data.\n",
    "\n",
    "10. **Biometric Data**: Fingerprints, facial recognition data, DNA sequences, and other data types that are unique to individuals.\n",
    "\n",
    "11. **Legal Data**: Information from court cases, police records, and other legal documents that contain sensitive personal information.\n",
    "\n",
    "12. **Transportation Data**: Travel records, vehicle tracking data, and other information related to an individual's travel patterns.\n",
    "\n",
    "13. **Utility Companies Data**: Information on individuals' utility usage patterns, which can be sensitive.\n",
    "\n",
    "14. **Internet Usage Data**: Browsing histories, search queries, and online behavior tracking.\n",
    "\n",
    "15. **Insurance Data**: Claim histories, risk assessments, and other personal data held by insurance companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20957b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
